{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lex_pol_classify.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XzMb4UhalAo5",
        "outputId": "5e7d7b79-f643-4c7f-f42b-7e2c5ea1fb5e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eug9H5DzlKgb",
        "outputId": "3c0a6f7c-3100-4308-862e-c32ae0a8249d"
      },
      "source": [
        "!pip install pyspark\n",
        "!pip install afinn"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/b0/9d6860891ab14a39d4bddf80ba26ce51c2f9dc4805e5c6978ac0472c120a/pyspark-3.1.1.tar.gz (212.3MB)\n",
            "\u001b[K     |████████████████████████████████| 212.3MB 68kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/b6/6a4fb90cd235dc8e265a6a2067f2a2c99f0d91787f06aca4bcf7c23f3f80/py4j-0.10.9-py2.py3-none-any.whl (198kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 20.2MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.1.1-py2.py3-none-any.whl size=212767604 sha256=88fec3885f825325ce67088117ba5f0b864d30a36ba45875de233018bcc67635\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/90/c0/01de724414ef122bd05f056541fb6a0ecf47c7ca655f8b3c0f\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9 pyspark-3.1.1\n",
            "Collecting afinn\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/86/e5/ffbb7ee3cca21ac6d310ac01944fb163c20030b45bda25421d725d8a859a/afinn-0.1.tar.gz (52kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 2.9MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: afinn\n",
            "  Building wheel for afinn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for afinn: filename=afinn-0.1-cp37-none-any.whl size=53451 sha256=09263675d35648910578485a8b2a0ea8d54a4159b0999bf1a7422eb83b41c92d\n",
            "  Stored in directory: /root/.cache/pip/wheels/b5/1c/de/428301f3333ca509dcf20ff358690eb23a1388fbcbbde008b2\n",
            "Successfully built afinn\n",
            "Installing collected packages: afinn\n",
            "Successfully installed afinn-0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sCkotik1liUw",
        "outputId": "8b075c9a-a7eb-4c6f-dbd6-7ced361071b9"
      },
      "source": [
        "from pyspark.sql import SparkSession as ss\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.ml.feature import MaxAbsScaler\n",
        "# from pyspark.ml.linalg import Vectors\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.sql.types import FloatType,StringType\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from afinn import Afinn\n",
        "from textblob import TextBlob\n",
        "import nltk\n",
        "nltk.download('vader_lexicon')\n",
        "spark = ss.builder.getOrCreate()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJf7brtalpyt"
      },
      "source": [
        "class lex_anal:\n",
        "  def __init__(self,file):\n",
        "      self.f = file\n",
        "      self.path_read = '/content/drive/My Drive/'\n",
        "      self.path_write = '/content/drive/My Drive/'\n",
        "      self.df = spark.read.option(\"header\",\"true\").csv(self.path_read+self.f,inferSchema = True,multiLine = True)\n",
        "  \n",
        "  def process(self):\n",
        "      print(\"PROCESSING SCORES\")\n",
        "      self.process_score()\n",
        "      print(\"SCALING\")\n",
        "      self.scale()\n",
        "      print(\"PROCESSING CLASS\")\n",
        "      self.process_class()\n",
        "      print(\"SAVING\")\n",
        "      self.save()\n",
        "  \n",
        "  \n",
        "  @staticmethod\n",
        "  @udf(returnType=FloatType())\n",
        "  def vader_pol(text):\n",
        "      vader = SentimentIntensityAnalyzer()\n",
        "      return dict(vader.polarity_scores(text))['compound']\n",
        "\n",
        "  @staticmethod\n",
        "  @udf(returnType=FloatType())\n",
        "  def afinn_pol(text):\n",
        "      af = Afinn()\n",
        "      return af.score(text)\n",
        "\n",
        "  @staticmethod\n",
        "  @udf(returnType = FloatType())\n",
        "  def blob_pol(text):\n",
        "      return TextBlob(text).polarity\n",
        "\n",
        "  def process_score(self):\n",
        "      self.df = self.df.withColumn('vader_score',lex_anal.vader_pol('pre_text_vader'))\n",
        "      self.df = self.df.withColumn('afinn_score',lex_anal.afinn_pol('pre_text_all_upd'))\n",
        "      self.df = self.df.withColumn('blob_score',lex_anal.blob_pol('pre_text_all_upd'))\n",
        "  \n",
        "  def get_df(self):\n",
        "      return self.df\n",
        "\n",
        "  def process_class(self):\n",
        "      self.df = self.df.withColumn('vader_class',lex_anal.classify('vader_score'))\n",
        "      self.df = self.df.withColumn('afinn_class',lex_anal.classify('afinn_score'))\n",
        "      self.df = self.df.withColumn('blob_class',lex_anal.classify('blob_score'))\n",
        "  \n",
        "  def save(self):\n",
        "      self.df.write.mode(\"overwrite\").option(\"header\",\"true\").csv(self.path_write+self.f+'_res')\n",
        "\n",
        "\n",
        "  def scale(self):\n",
        "      columns_to_scale = [\"afinn_score\"]\n",
        "      assemblers = [VectorAssembler(inputCols=[col], outputCol=col + \"_vec\") for col in columns_to_scale]\n",
        "      scalers = [MaxAbsScaler(inputCol=col + \"_vec\", outputCol=col + \"_scaled\") for col in columns_to_scale]\n",
        "      pipeline = Pipeline(stages=assemblers + scalers)\n",
        "      scalerModel = pipeline.fit(self.df)\n",
        "      scaledData = scalerModel.transform(self.df)\n",
        "    # scaledData = scaledData.drop('afinn_score_vec')\n",
        "      unlist = udf(lambda x: float(list(x)[0]), FloatType())\n",
        "      scaledData = scaledData.withColumn('afinn_score_scaled_f',unlist('afinn_score_scaled'))\n",
        "      scaledData = scaledData.drop('afinn_score_scaled','afinn_score_vec','afinn_score')\n",
        "      scaledData = scaledData.withColumnRenamed('afinn_score_scaled_f','afinn_score')\n",
        "      self.df = scaledData\n",
        "\n",
        "\n",
        "  @staticmethod\n",
        "  @udf(returnType = StringType())\n",
        "  def classify(score):\n",
        "      if score>0.5:\n",
        "          return 'VPos'\n",
        "      if score>0 and score<=0.5:\n",
        "          return 'Pos'\n",
        "      if score<0 and score>=-0.5:\n",
        "          return 'Neg'\n",
        "      if score<-0.5:\n",
        "          return 'VNeg'\n",
        "      return 'Neu'\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aLGtxNp2lvuL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1bf3514-d691-4ad2-f1aa-5844ae4fd160"
      },
      "source": [
        "import os\n",
        "os.chdir('/content/drive/My Drive/')\n",
        "os.listdir()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Untitled0.ipynb',\n",
              " 'Colab Notebooks',\n",
              " 'part-00005-acd68a15-c739-4cf0-a97b-c470b4925346-c000.csv',\n",
              " 'part-5',\n",
              " 'total_combined_text_and_ids_preprocessed.csv',\n",
              " 'total_combined_text_and_ids_preprocessed_upd']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQVetF5kmDsq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90ccd3c8-dd16-4c9d-cb68-5f3b2c5dca74"
      },
      "source": [
        "os.chdir('/content/drive/My Drive/sentiment140_cleaned_csv_preprocessed')\n",
        "files = [file for file in os.listdir() if file.endswith('.csv')]\n",
        "files"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['part-00000-acd68a15-c739-4cf0-a97b-c470b4925346-c000.csv',\n",
              " 'part-00001-acd68a15-c739-4cf0-a97b-c470b4925346-c000.csv',\n",
              " 'part-00003-acd68a15-c739-4cf0-a97b-c470b4925346-c000.csv',\n",
              " 'part-00002-acd68a15-c739-4cf0-a97b-c470b4925346-c000.csv',\n",
              " 'part-00004-acd68a15-c739-4cf0-a97b-c470b4925346-c000.csv',\n",
              " 'part-00005-acd68a15-c739-4cf0-a97b-c470b4925346-c000.csv']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlIawSx29kSj"
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('/content/drive/My Drive/total_combined_text_and_ids_preprocessed.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CTfANhnr_Foq",
        "outputId": "822a4327-c6ed-4ae3-e71d-06004e979538"
      },
      "source": [
        "l = lex_anal('total_combined_text_and_ids_preprocessed_upd')\n",
        "l.process()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PROCESSING SCORES\n",
            "SCALING\n",
            "PROCESSING CLASS\n",
            "SAVING\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EAnLd0Ec5N1s"
      },
      "source": [
        "df.drop_duplicates('pre_text_all_upd',inplace = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3Ampsso7s26"
      },
      "source": [
        "df.to_csv('/content/drive/My Drive/total_combined_text_and_ids_preprocessed.csv',index = False,header = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qG5VuoGw7tRa"
      },
      "source": [
        "from pyspark.sql.types import *\n",
        "mySchema = StructType([ StructField(\"text\", StringType(), True),StructField(\"pre_text_vader\", StringType(), True),StructField(\"pre_text_all_upd\", StringType(), True)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jobiCJdP95pM"
      },
      "source": [
        "df_sp = spark.createDataFrame(df,schema = mySchema)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXqndaAO-Bxw"
      },
      "source": [
        "df_sp.write.format('csv').option(\"header\",\"true\").save(\"/content/drive/My Drive/total_combined_text_and_ids_preprocessed_upd\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5ltMxZG-qhZ"
      },
      "source": [
        "df_sp1 = spark.read.option(\"header\",\"true\").csv(\"/content/drive/My Drive/total_combined_text_and_ids_preprocessed_upd_res\",inferSchema = True,multiLine = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VCa4HV6u-0lA",
        "outputId": "2155ebce-1631-41a2-ce0f-f3a67febf423"
      },
      "source": [
        "df_sp1.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+--------------------+--------------------+\n",
            "|                text|      pre_text_vader|    pre_text_all_upd|\n",
            "+--------------------+--------------------+--------------------+\n",
            "|Unfortunately the...|Unfortunately the...|unfortunately the...|\n",
            "|@Russo_Brothers #...|best way to stay ...|best way to stay ...|\n",
            "|#NewYorkCity\n",
            "WE A...|WE ARE OPEN MONDA...|we are open monda...|\n",
            "|It’s a blessing w...|Its a blessing wh...|its a blessing wh...|\n",
            "|@rejectedjokes @n...|Are you sure the ...|are you sure the ...|\n",
            "|@didikins4life tr...|trumps wants to c...|trumps wants to c...|\n",
            "|“To be a function...|To be a function ...|to be a function ...|\n",
            "|@Iheartnoise @Bad...|It lives. For worse.|  it lives for worse|\n",
            "|I think we can al...|I think we can al...|i think we can al...|\n",
            "|@XAn0nXm0uSX @ber...|ppl got to b dens...|ppl got to b dens...|\n",
            "|@BoozyBadger Well...|Well I use Steam ...|well i use steam ...|\n",
            "|https://t.co/5WyK...|all different typ...|all different typ...|\n",
            "|Thank you to all ...|hank you to all o...|hank you to all o...|\n",
            "|@PedagogyOrNah @i...|I am reading Amer...|i am reading amer...|\n",
            "|@Phoenixrush85 @R...|I got the basemen...|  i got the basement|\n",
            "|@BenjaminPDixon M...|Meanwhile, she wi...|meanwhile she wil...|\n",
            "|If you eat banana...|If you eat banana...|if you eat banana...|\n",
            "|Together we are g...|ogether we are go...|ogether we are go...|\n",
            "|Check it out on S...|Check it out on S...|check it out on s...|\n",
            "|Thanks @365datasc...|hanks , for the o...|hanks  for the op...|\n",
            "+--------------------+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}