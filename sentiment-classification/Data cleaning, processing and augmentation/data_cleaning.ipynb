{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "data_cleaning.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0SntXeJAhkds",
        "outputId": "30a2e6d5-4dc6-4edb-f012-8fe3d86e7106"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hS3gNixnhqK8"
      },
      "source": [
        "import pandas as pd\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKjML3Cih3bu"
      },
      "source": [
        "df = pd.read_excel('/drive/My Drive/scraped.xlsx')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "osegS-VBh9l8",
        "outputId": "5b8f178e-5c5a-4c4c-f216-400d395fcdff"
      },
      "source": [
        "df.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "username          0\n",
              "description     120\n",
              "location        302\n",
              "following         0\n",
              "followers         0\n",
              "totaltweets       0\n",
              "retweetcount      0\n",
              "text              0\n",
              "hashtags          0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "MITsSdZwiEa0",
        "outputId": "628268d1-e5af-4bd4-d732-9b684ca8ea06"
      },
      "source": [
        "df.drop(['description','location'],axis = 1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>username</th>\n",
              "      <th>following</th>\n",
              "      <th>followers</th>\n",
              "      <th>totaltweets</th>\n",
              "      <th>retweetcount</th>\n",
              "      <th>text</th>\n",
              "      <th>hashtags</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>PANGKAKISTOS</td>\n",
              "      <td>2451</td>\n",
              "      <td>2248</td>\n",
              "      <td>83222</td>\n",
              "      <td>1</td>\n",
              "      <td>HIGHWAY TO HELL...... #Covid_19 #COVID19 #Coro...</td>\n",
              "      <td>['coronavirus', 'CoronavirusOutbreak', 'Corona...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>corona_scotnews</td>\n",
              "      <td>0</td>\n",
              "      <td>614</td>\n",
              "      <td>15420</td>\n",
              "      <td>0</td>\n",
              "      <td>Nearly half of north-east over-80s given first...</td>\n",
              "      <td>['coronavirus', 'covid19']</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>DailyNews_lk</td>\n",
              "      <td>80</td>\n",
              "      <td>6255</td>\n",
              "      <td>23093</td>\n",
              "      <td>0</td>\n",
              "      <td>353 COVID-19 positive cases reported\\n\\n#COVID...</td>\n",
              "      <td>['LakeHouseDigital', 'coronavirus', 'SriLanka'...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>sadia_mufti</td>\n",
              "      <td>102</td>\n",
              "      <td>2559</td>\n",
              "      <td>1256</td>\n",
              "      <td>0</td>\n",
              "      <td>Seems Corona is in love with her üòÇShocking! Wo...</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Ociocentez</td>\n",
              "      <td>1358</td>\n",
              "      <td>293</td>\n",
              "      <td>45421</td>\n",
              "      <td>9</td>\n",
              "      <td>This week, the WHO said the world is on the br...</td>\n",
              "      <td>['corona', 'coronavirus', 'pandemic', 'vaccine...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>AsharqPlus</td>\n",
              "      <td>9</td>\n",
              "      <td>6</td>\n",
              "      <td>72</td>\n",
              "      <td>0</td>\n",
              "      <td>Kazakhstan plans to vaccinate about 6 million ...</td>\n",
              "      <td>['kazakhstan', 'corona', 'alarabiya', 'covid_19']</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>facemask24_shop</td>\n",
              "      <td>25</td>\n",
              "      <td>0</td>\n",
              "      <td>30</td>\n",
              "      <td>0</td>\n",
              "      <td>Stay healthy!‚†Ä\\n‚†Ä\\nhttps://t.co/FQd6DBbs5Q‚†Ä\\n‚†Ä...</td>\n",
              "      <td>['helpothers', 'covid2020', 'corona', 'ootd', ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>AmandaR14821525</td>\n",
              "      <td>23</td>\n",
              "      <td>0</td>\n",
              "      <td>31</td>\n",
              "      <td>0</td>\n",
              "      <td>coronavirus infections are surging to unpreced...</td>\n",
              "      <td>['BiggBoss14', 'economy', 'CoronaVaccine', 'Co...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>Sparkymannz</td>\n",
              "      <td>865</td>\n",
              "      <td>502</td>\n",
              "      <td>31439</td>\n",
              "      <td>112</td>\n",
              "      <td>Over 100 Scientists, Doctors, &amp;amp; Leading Au...</td>\n",
              "      <td>['corona', 'vitaminaD', 'Coronavirus', 'covid'...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>530postscript</td>\n",
              "      <td>194</td>\n",
              "      <td>391</td>\n",
              "      <td>38338</td>\n",
              "      <td>0</td>\n",
              "      <td>#Corona #coronavirus #COVID„Éº19 \\nCovid-19 has ...</td>\n",
              "      <td>['coronavirus', 'Corona', 'COVID„Éº19']</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows √ó 7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            username  ...                                           hashtags\n",
              "0       PANGKAKISTOS  ...  ['coronavirus', 'CoronavirusOutbreak', 'Corona...\n",
              "1    corona_scotnews  ...                         ['coronavirus', 'covid19']\n",
              "2       DailyNews_lk  ...  ['LakeHouseDigital', 'coronavirus', 'SriLanka'...\n",
              "3        sadia_mufti  ...                                                 []\n",
              "4         Ociocentez  ...  ['corona', 'coronavirus', 'pandemic', 'vaccine...\n",
              "..               ...  ...                                                ...\n",
              "995       AsharqPlus  ...  ['kazakhstan', 'corona', 'alarabiya', 'covid_19']\n",
              "996  facemask24_shop  ...  ['helpothers', 'covid2020', 'corona', 'ootd', ...\n",
              "997  AmandaR14821525  ...  ['BiggBoss14', 'economy', 'CoronaVaccine', 'Co...\n",
              "998      Sparkymannz  ...  ['corona', 'vitaminaD', 'Coronavirus', 'covid'...\n",
              "999    530postscript  ...              ['coronavirus', 'Corona', 'COVID„Éº19']\n",
              "\n",
              "[1000 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X9AasiNhtDDh",
        "outputId": "3460bca6-8551-46c2-9d4d-4549ac32eedb"
      },
      "source": [
        "df.dtypes"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "username        object\n",
              "description     object\n",
              "location        object\n",
              "following       object\n",
              "followers       object\n",
              "totaltweets     object\n",
              "retweetcount    object\n",
              "text            object\n",
              "hashtags        object\n",
              "dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ww3WvnettFSq"
      },
      "source": [
        "df = df.astype('object')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H_2LwMFTtSju",
        "outputId": "8d37bbbe-e1fe-49cd-e5b9-727f71c5a576"
      },
      "source": [
        "len(df[\"hashtags\"][0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "71"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gaDHoB9hiuNR"
      },
      "source": [
        "class tags:\n",
        "  def __init__(self,df):\n",
        "    self.df = df\n",
        "\n",
        "\n",
        "  def process(self):\n",
        "    res = []\n",
        "    for i in range(len(self.df)):\n",
        "      self.df[\"hashtags\"][i] = self.preprocess(self.df['hashtags'][i])\n",
        "    return self.df\n",
        "\n",
        "\n",
        "  def preprocess(self,s):\n",
        "    s = s.replace(\"'\", \"\")\n",
        "    s = s.replace(\"[\",\"\")\n",
        "    s = s.replace(\"]\",\"\")\n",
        "    s = s.replace(\" \",\"\")\n",
        "    \n",
        "    return s\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fOiqPg9ok2L8"
      },
      "source": [
        "t = tags(df)\n",
        "df1 = t.process()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwsVv564mSO1"
      },
      "source": [
        "#for finding intersection/union of hashtags for all reviews\n",
        "# res = []\n",
        "# for i in range(len(df1)-1):\n",
        "#   res = list(set(res) | set(df1[i]) | set(df1[i+1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1qQOxYxwF6F"
      },
      "source": [
        "df1[\"hashtags\"]=df1[\"hashtags\"].apply(lambda x: x.lower())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6A5yPRiD6Y5Z"
      },
      "source": [
        "from collections import Counter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRJYRkr66yAr"
      },
      "source": [
        "res = []\n",
        "for i in range(len(df1)):\n",
        "  res.extend(df1[\"hashtags\"][i].split(\",\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hb1p9PMu9Yl6"
      },
      "source": [
        "from collections import defaultdict \n",
        "hashtags_count = defaultdict(int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O9uO_DeE7-HY"
      },
      "source": [
        "for i in range(len(df1)):\n",
        "  if len(df1[\"hashtags\"][i]) == 0:\n",
        "    hashtags_count[0]+=1\n",
        "    continue\n",
        "  count = len(df1[\"hashtags\"][i].split(\",\"))\n",
        "  hashtags_count[count]+=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JO-arn8r-dqJ"
      },
      "source": [
        "def percent(part,whole):\n",
        "  return \"%.2f\" %((part/whole)*100)\n",
        "\n",
        "\n",
        "def hashtags_analysis(hashtags_count):\n",
        "  res = []\n",
        "  for key in sorted(hashtags_count.keys()):\n",
        "    s = '{0} tweets with {1} hashtags ({2}%)'.format(hashtags_count[key],key,percent(hashtags_count[key],len(df1)))\n",
        "    res.append(s)\n",
        "  return res"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wmoTGQ9aBJqY"
      },
      "source": [
        "r = hashtags_analysis(hashtags_count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NlUq0lFwBMFv",
        "outputId": "0f787b4c-5152-428e-ae9f-415896eb81b4"
      },
      "source": [
        "r"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['217 tweets with 0 hashtags (21.70%)',\n",
              " '32 tweets with 1 hashtags (3.20%)',\n",
              " '58 tweets with 2 hashtags (5.80%)',\n",
              " '45 tweets with 3 hashtags (4.50%)',\n",
              " '68 tweets with 4 hashtags (6.80%)',\n",
              " '104 tweets with 5 hashtags (10.40%)',\n",
              " '40 tweets with 6 hashtags (4.00%)',\n",
              " '37 tweets with 7 hashtags (3.70%)',\n",
              " '181 tweets with 8 hashtags (18.10%)',\n",
              " '25 tweets with 9 hashtags (2.50%)',\n",
              " '26 tweets with 10 hashtags (2.60%)',\n",
              " '41 tweets with 11 hashtags (4.10%)',\n",
              " '44 tweets with 12 hashtags (4.40%)',\n",
              " '9 tweets with 13 hashtags (0.90%)',\n",
              " '18 tweets with 14 hashtags (1.80%)',\n",
              " '7 tweets with 15 hashtags (0.70%)',\n",
              " '16 tweets with 16 hashtags (1.60%)',\n",
              " '9 tweets with 17 hashtags (0.90%)',\n",
              " '4 tweets with 18 hashtags (0.40%)',\n",
              " '11 tweets with 19 hashtags (1.10%)',\n",
              " '4 tweets with 20 hashtags (0.40%)',\n",
              " '2 tweets with 21 hashtags (0.20%)',\n",
              " '1 tweets with 23 hashtags (0.10%)',\n",
              " '1 tweets with 27 hashtags (0.10%)']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NLexQHKF7F2e",
        "outputId": "55abac4b-ac38-4858-8dbc-dc8eaec39124"
      },
      "source": [
        "print(len(df1.index))#1000\n",
        "serlis=df1.duplicated().tolist()\n",
        "print(serlis.count(True))#11\n",
        "serlis=df1.duplicated(['text']).tolist()\n",
        "print(serlis.count(True))#340"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1000\n",
            "11\n",
            "340\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dv8TR7JZ6UYO"
      },
      "source": [
        "df=df1.drop_duplicates(['text'])\n",
        "df.reset_index(inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        },
        "id": "Rh5it_vj6ZNp",
        "outputId": "bce9a9b7-4887-4745-d32c-2af3398b70f4"
      },
      "source": [
        "df.drop(['description','location'],axis = 1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>username</th>\n",
              "      <th>following</th>\n",
              "      <th>followers</th>\n",
              "      <th>totaltweets</th>\n",
              "      <th>retweetcount</th>\n",
              "      <th>text</th>\n",
              "      <th>hashtags</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>PANGKAKISTOS</td>\n",
              "      <td>2451</td>\n",
              "      <td>2248</td>\n",
              "      <td>83222</td>\n",
              "      <td>1</td>\n",
              "      <td>HIGHWAY TO HELL...... #Covid_19 #COVID19 #Coro...</td>\n",
              "      <td>coronavirus,coronavirusoutbreak,corona,covid_1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>corona_scotnews</td>\n",
              "      <td>0</td>\n",
              "      <td>614</td>\n",
              "      <td>15420</td>\n",
              "      <td>0</td>\n",
              "      <td>Nearly half of north-east over-80s given first...</td>\n",
              "      <td>coronavirus,covid19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>DailyNews_lk</td>\n",
              "      <td>80</td>\n",
              "      <td>6255</td>\n",
              "      <td>23093</td>\n",
              "      <td>0</td>\n",
              "      <td>353 COVID-19 positive cases reported\\n\\n#COVID...</td>\n",
              "      <td>lakehousedigital,coronavirus,srilanka,covid,co...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>sadia_mufti</td>\n",
              "      <td>102</td>\n",
              "      <td>2559</td>\n",
              "      <td>1256</td>\n",
              "      <td>0</td>\n",
              "      <td>Seems Corona is in love with her üòÇShocking! Wo...</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>Ociocentez</td>\n",
              "      <td>1358</td>\n",
              "      <td>293</td>\n",
              "      <td>45421</td>\n",
              "      <td>9</td>\n",
              "      <td>This week, the WHO said the world is on the br...</td>\n",
              "      <td>corona,coronavirus,pandemic,vaccine,inequality</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>655</th>\n",
              "      <td>993</td>\n",
              "      <td>DuaSahildua1</td>\n",
              "      <td>69</td>\n",
              "      <td>7</td>\n",
              "      <td>2676</td>\n",
              "      <td>0</td>\n",
              "      <td>@ndtv Check out this blog to know what are the...</td>\n",
              "      <td>covid__19,vaccinationdrive,covid19ireland,vacc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>656</th>\n",
              "      <td>995</td>\n",
              "      <td>AsharqPlus</td>\n",
              "      <td>9</td>\n",
              "      <td>6</td>\n",
              "      <td>72</td>\n",
              "      <td>0</td>\n",
              "      <td>Kazakhstan plans to vaccinate about 6 million ...</td>\n",
              "      <td>kazakhstan,corona,alarabiya,covid_19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>657</th>\n",
              "      <td>996</td>\n",
              "      <td>facemask24_shop</td>\n",
              "      <td>25</td>\n",
              "      <td>0</td>\n",
              "      <td>30</td>\n",
              "      <td>0</td>\n",
              "      <td>Stay healthy!‚†Ä\\n‚†Ä\\nhttps://t.co/FQd6DBbs5Q‚†Ä\\n‚†Ä...</td>\n",
              "      <td>helpothers,covid2020,corona,ootd,coronavirus,h...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>658</th>\n",
              "      <td>997</td>\n",
              "      <td>AmandaR14821525</td>\n",
              "      <td>23</td>\n",
              "      <td>0</td>\n",
              "      <td>31</td>\n",
              "      <td>0</td>\n",
              "      <td>coronavirus infections are surging to unpreced...</td>\n",
              "      <td>biggboss14,economy,coronavaccine,corona,stocks...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>659</th>\n",
              "      <td>999</td>\n",
              "      <td>530postscript</td>\n",
              "      <td>194</td>\n",
              "      <td>391</td>\n",
              "      <td>38338</td>\n",
              "      <td>0</td>\n",
              "      <td>#Corona #coronavirus #COVID„Éº19 \\nCovid-19 has ...</td>\n",
              "      <td>coronavirus,corona,covid„Éº19</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>660 rows √ó 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     index  ...                                           hashtags\n",
              "0        0  ...  coronavirus,coronavirusoutbreak,corona,covid_1...\n",
              "1        1  ...                                coronavirus,covid19\n",
              "2        2  ...  lakehousedigital,coronavirus,srilanka,covid,co...\n",
              "3        3  ...                                                   \n",
              "4        4  ...     corona,coronavirus,pandemic,vaccine,inequality\n",
              "..     ...  ...                                                ...\n",
              "655    993  ...  covid__19,vaccinationdrive,covid19ireland,vacc...\n",
              "656    995  ...               kazakhstan,corona,alarabiya,covid_19\n",
              "657    996  ...  helpothers,covid2020,corona,ootd,coronavirus,h...\n",
              "658    997  ...  biggboss14,economy,coronavaccine,corona,stocks...\n",
              "659    999  ...                        coronavirus,corona,covid„Éº19\n",
              "\n",
              "[660 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 767
        },
        "id": "3gcYLWuO6o94",
        "outputId": "fb5e9435-e86d-41c4-9ef5-0397f4d6a08d"
      },
      "source": [
        "df.head(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>username</th>\n",
              "      <th>description</th>\n",
              "      <th>location</th>\n",
              "      <th>following</th>\n",
              "      <th>followers</th>\n",
              "      <th>totaltweets</th>\n",
              "      <th>retweetcount</th>\n",
              "      <th>text</th>\n",
              "      <th>hashtags</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>PANGKAKISTOS</td>\n",
              "      <td>Œ£Œ§ŒóŒù Œ£ŒóŒúŒïŒ°ŒôŒùŒó ŒöŒ°ŒôŒ£Œó Œ§ŒóŒ£ ŒßŒ©Œ°ŒëŒ£ Œó ŒóŒõŒôŒòŒôŒüŒ§ŒóŒ§Œë ŒöŒëŒô...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2451</td>\n",
              "      <td>2248</td>\n",
              "      <td>83222</td>\n",
              "      <td>1</td>\n",
              "      <td>HIGHWAY TO HELL...... #Covid_19 #COVID19 #Coro...</td>\n",
              "      <td>coronavirus,coronavirusoutbreak,corona,covid_1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>corona_scotnews</td>\n",
              "      <td>The latest #coronavirus #COVID19 news from @dc...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>614</td>\n",
              "      <td>15420</td>\n",
              "      <td>0</td>\n",
              "      <td>Nearly half of north-east over-80s given first...</td>\n",
              "      <td>coronavirus,covid19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>DailyNews_lk</td>\n",
              "      <td>Sri Lanka's premier daily newspaper, since 1918.</td>\n",
              "      <td>Lake House, Sri Lanka</td>\n",
              "      <td>80</td>\n",
              "      <td>6255</td>\n",
              "      <td>23093</td>\n",
              "      <td>0</td>\n",
              "      <td>353 COVID-19 positive cases reported\\n\\n#COVID...</td>\n",
              "      <td>lakehousedigital,coronavirus,srilanka,covid,co...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>sadia_mufti</td>\n",
              "      <td>Fashion Designer/Stylist | Owner - Hangers The...</td>\n",
              "      <td>Kashmir</td>\n",
              "      <td>102</td>\n",
              "      <td>2559</td>\n",
              "      <td>1256</td>\n",
              "      <td>0</td>\n",
              "      <td>Seems Corona is in love with her üòÇShocking! Wo...</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>Ociocentez</td>\n",
              "      <td>¬°Que a todo el mundo le vaya bien!</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1358</td>\n",
              "      <td>293</td>\n",
              "      <td>45421</td>\n",
              "      <td>9</td>\n",
              "      <td>This week, the WHO said the world is on the br...</td>\n",
              "      <td>corona,coronavirus,pandemic,vaccine,inequality</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5</td>\n",
              "      <td>krishna444_test</td>\n",
              "      <td>Alerting the world, with the current happening...</td>\n",
              "      <td>Karlsruhe, Germany</td>\n",
              "      <td>36</td>\n",
              "      <td>95</td>\n",
              "      <td>10370</td>\n",
              "      <td>0</td>\n",
              "      <td>#Corona Info for #USA:\\nNew Cases: 2600\\nToday...</td>\n",
              "      <td>covid19,usa,coronavirus,staysafe,corona,telegr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>6</td>\n",
              "      <td>corona_tweet</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>248</td>\n",
              "      <td>33611</td>\n",
              "      <td>24</td>\n",
              "      <td>#COVID19: Senior doctors have called on Englan...</td>\n",
              "      <td>coronavirus,covid19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>7</td>\n",
              "      <td>AnthonyEhilebo</td>\n",
              "      <td>Papa Adesuwa and Arese!\\nBuckingham Alumnus\\nB...</td>\n",
              "      <td>Abuja</td>\n",
              "      <td>5203</td>\n",
              "      <td>24146</td>\n",
              "      <td>184219</td>\n",
              "      <td>1</td>\n",
              "      <td>Staying in business now, involves 3 linked ste...</td>\n",
              "      <td>businesstip,coronavirus,covidsecondwave,busine...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>9</td>\n",
              "      <td>EightStats</td>\n",
              "      <td>Statistics of topics, which already changed or...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>16</td>\n",
              "      <td>0</td>\n",
              "      <td>Top USA States by Number of Coronavirus Vaccin...</td>\n",
              "      <td>moderna,comirnaty,lockdown,vaccination,pfizer,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>10</td>\n",
              "      <td>corona_scotnews</td>\n",
              "      <td>The latest #coronavirus #COVID19 news from @dc...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>614</td>\n",
              "      <td>15420</td>\n",
              "      <td>0</td>\n",
              "      <td>Nearly half of north-east over-80s given first...</td>\n",
              "      <td>coronavirus,covid19</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   index  ...                                           hashtags\n",
              "0      0  ...  coronavirus,coronavirusoutbreak,corona,covid_1...\n",
              "1      1  ...                                coronavirus,covid19\n",
              "2      2  ...  lakehousedigital,coronavirus,srilanka,covid,co...\n",
              "3      3  ...                                                   \n",
              "4      4  ...     corona,coronavirus,pandemic,vaccine,inequality\n",
              "5      5  ...  covid19,usa,coronavirus,staysafe,corona,telegr...\n",
              "6      6  ...                                coronavirus,covid19\n",
              "7      7  ...  businesstip,coronavirus,covidsecondwave,busine...\n",
              "8      9  ...  moderna,comirnaty,lockdown,vaccination,pfizer,...\n",
              "9     10  ...                                coronavirus,covid19\n",
              "\n",
              "[10 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6vtBV0c8yDp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gwb8IyKv79Y6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAMA7OjW7G5k"
      },
      "source": [
        "import re\n",
        "for i in range(len(df)):\n",
        "    txt = df['text'][i]\n",
        "    txt=re.sub(r'@[A-Z0-9a-z_:]+','',txt)#replace username-tags\n",
        "    txt=re.sub(r'^[RT]+','',txt)#replace RT-tags\n",
        "    txt = re.sub('https?://[A-Za-z0-9./]+','',txt)#replace URLs\n",
        "    txt=re.sub(\"[^a-zA-Z]\", \" \",txt)#replace hashtags\n",
        "    \n",
        "    df.at[i,'text']=txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_xsQhxK26x03",
        "outputId": "d78df543-8ad2-4415-a032-2b3672cfa2a5"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('universal_tagset')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3KZdDEQV-DxL",
        "outputId": "48b6e2ec-ae53-4bc3-e709-40ba10b58896"
      },
      "source": [
        "df_copy = df.copy()\n",
        "from nltk.corpus import sentiwordnet as swn\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem import PorterStemmer\n",
        "nltk.download('sentiwordnet')\n",
        "nltk.download('wordnet')\n",
        "# for i in range(len(df_copy.index)):\n",
        "#         text = df_copy['text'][i]\n",
        "#         tokens = nltk.word_tokenize(text)\n",
        "#         tagged_sent = nltk.pos_tag(tokens)\n",
        "#         store_it = [(word, nltk.map_tag('en-ptb', 'universal', tag)) for word, tag in tagged_sent]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package sentiwordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgfWAtqS9i6l"
      },
      "source": [
        " def pos_senti_swn(df_copy):#takes\n",
        "    li_swn=[]\n",
        "    li_swn_pos=[]\n",
        "    li_swn_neg=[]\n",
        "    missing_words=[]\n",
        "    for i in range(len(df_copy.index)):\n",
        "        text = df_copy['text'][i]\n",
        "        tokens = nltk.word_tokenize(text)\n",
        "        tagged_sent = nltk.pos_tag(tokens)\n",
        "        store_it = [(word, nltk.map_tag('en-ptb', 'universal', tag)) for word, tag in tagged_sent]\n",
        "        #print(\"Tagged Parts of Speech:\",store_it)\n",
        "        lem = WordNetLemmatizer()\n",
        "        pstem = PorterStemmer()\n",
        "        pos_total=0\n",
        "        neg_total=0\n",
        "        for word,tag in store_it:\n",
        "            if(tag=='NOUN'):\n",
        "                tag='n'\n",
        "            elif(tag=='VERB'):\n",
        "                tag='v'\n",
        "            elif(tag=='ADJ'):\n",
        "                tag='a'\n",
        "            elif(tag=='ADV'):\n",
        "                tag = 'r'\n",
        "            else:\n",
        "                tag='nothing'\n",
        "\n",
        "            if(tag!='nothing'):\n",
        "                concat = word+'.'+tag+'.01'\n",
        "                try:\n",
        "                    this_word_pos=swn.senti_synset(concat).pos_score()\n",
        "                    this_word_neg=swn.senti_synset(concat).neg_score()\n",
        "                    #print(word,tag,':',this_word_pos,this_word_neg)\n",
        "                except Exception as e:\n",
        "                    wor = lem.lemmatize(word)\n",
        "                    concat = wor+'.'+tag+'.01'\n",
        "                    # Checking if there's a possiblity of lemmatized word be accepted into SWN corpus\n",
        "                    try:\n",
        "                        this_word_pos=swn.senti_synset(concat).pos_score()\n",
        "                        this_word_neg=swn.senti_synset(concat).neg_score()\n",
        "                    except Exception as e:\n",
        "                        wor = pstem.stem(word)\n",
        "                        concat = wor+'.'+tag+'.01'\n",
        "                        # Checking if there's a possiblity of lemmatized word be accepted\n",
        "                        try:\n",
        "                            this_word_pos=swn.senti_synset(concat).pos_score()\n",
        "                            this_word_neg=swn.senti_synset(concat).neg_score()\n",
        "                        except:\n",
        "                            missing_words.append(word)\n",
        "                            continue\n",
        "                pos_total+=this_word_pos\n",
        "                neg_total+=this_word_neg\n",
        "        li_swn_pos.append(pos_total)\n",
        "        li_swn_neg.append(neg_total)\n",
        "\n",
        "        if(pos_total!=0 or neg_total!=0):\n",
        "            if(pos_total>neg_total):\n",
        "                li_swn.append(1)\n",
        "            else:\n",
        "                li_swn.append(-1)\n",
        "        else:\n",
        "            li_swn.append(0)\n",
        "    df_copy.insert(5,\"pos_score\",li_swn_pos,True)\n",
        "    df_copy.insert(6,\"neg_score\",li_swn_neg,True)\n",
        "    df_copy.insert(7,\"sent_score\",li_swn,True)\n",
        "    return df_copy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7r0_Cl_Z-z1r"
      },
      "source": [
        "df_lbld = pos_senti_swn(df_copy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgwEGAgK-x7p"
      },
      "source": [
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "class cleaning:\n",
        "  def __init__(self,df):\n",
        "    self.df = df\n",
        "    self.li_swn=[]\n",
        "    self.li_swn_pos=[]\n",
        "    self.li_swn_neg=[]\n",
        "    self.missing_words=[]\n",
        "    self.lem = WordNetLemmatizer()\n",
        "    self.pstem = PorterStemmer()\n",
        "    self.stopwords = stopwords.words('english')\n",
        "  \n",
        "  def process(self):\n",
        "    self.clean()\n",
        "    # self.pos_tags_swn()\n",
        "    # self.pos_tag_after_cleaning()\n",
        "    # self.modify_df()\n",
        "\n",
        "  # def modify_df(self):\n",
        "  #   self.df.insert(5,\"pos_score\",self.li_swn_pos,True)\n",
        "  #   self.df.insert(6,\"neg_score\",self.li_swn_neg,True)\n",
        "  #   self.df.insert(7,\"sent_score\",self.li_swn,True)\n",
        "\n",
        "  def get_df(self):\n",
        "    return self.df\n",
        "\n",
        "  # def clean(self,s):\n",
        "  #   for i in range(len(self.df)):\n",
        "  #     txt = self.df['text'][i]\n",
        "  #     txt=re.sub(r'@[A-Z0-9a-z_:]+','',txt)#replace username-tags\n",
        "  #     txt=re.sub(r'^[RT]+','',txt)#replace RT-tags\n",
        "  #     txt = re.sub('https?://[A-Za-z0-9./]+','',txt)#replace URLs\n",
        "  #     txt=re.sub(\"[^a-zA-Z]\", \" \",txt)#replace hashtags\n",
        "      \n",
        "  #     self.df.at[i,'text']=txt\n",
        "\n",
        "  #   if s.lower() != 'f':\n",
        "  #     return\n",
        "\n",
        "  #     for i in range(len(self.df)):\n",
        "  #       text = self.df['text'][i]\n",
        "  #       tokens = nltk.word_tokenize(text)\n",
        "  #       tokens = [word for word in tokens if word not in self.stopwords]\n",
        "  #       for j in range(len(tokens)):\n",
        "  #           tokens[j] = self.lem.lemmatize(tokens[j])\n",
        "  #           tokens[j] = self.pstem.stem(tokens[j])\n",
        "\n",
        "  #       tokens_sent=' '.join(tokens)\n",
        "  #       self.df.at[i,\"text\"] = tokens_sent\n",
        "\n",
        "  # def pos_tag_after_cleaning(self):\n",
        "  #   for i in range(len(self.df)):\n",
        "  #     text = self.df['text'][i]\n",
        "  #     tokens = nltk.word_tokenize(text)\n",
        "  #     tokens = [word for word in tokens if word not in self.stopwords]\n",
        "  #     for j in range(len(tokens)):\n",
        "  #         tokens[j] = self.lem.lemmatize(tokens[j])\n",
        "  #         tokens[j] = self.pstem.stem(tokens[j])\n",
        "\n",
        "  #     tokens_sent=' '.join(tokens)\n",
        "  #     self.df.at[i,\"text\"] = tokens_sent\n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "  # def pos_tags_swn(self):\n",
        "    \n",
        "  #   for i in range(len(self.df.index)):\n",
        "  #       text = self.df['text'][i]\n",
        "  #       tokens = nltk.word_tokenize(text)\n",
        "  #       tagged_sent = nltk.pos_tag(tokens)\n",
        "  #       store_it = [(word, nltk.map_tag('en-ptb', 'universal', tag)) for word, tag in tagged_sent]\n",
        "  #       #print(\"Tagged Parts of Speech:\",store_it)\n",
        "        \n",
        "  #       pos_total=0\n",
        "  #       neg_total=0\n",
        "  #       for word,tag in store_it:\n",
        "  #           if(tag=='NOUN'):\n",
        "  #               tag='n'\n",
        "  #           elif(tag=='VERB'):\n",
        "  #               tag='v'\n",
        "  #           elif(tag=='ADJ'):\n",
        "  #               tag='a'\n",
        "  #           elif(tag=='ADV'):\n",
        "  #               tag = 'r'\n",
        "  #           else:\n",
        "  #               tag='nothing'\n",
        "\n",
        "  #           if(tag!='nothing'):\n",
        "  #               concat = word+'.'+tag+'.01'\n",
        "  #               try:\n",
        "  #                   this_word_pos=swn.senti_synset(concat).pos_score()\n",
        "  #                   this_word_neg=swn.senti_synset(concat).neg_score()\n",
        "  #                   #print(word,tag,':',this_word_pos,this_word_neg)\n",
        "  #               except Exception as e:\n",
        "  #                   wor = self.lem.lemmatize(word)\n",
        "  #                   concat = wor+'.'+tag+'.01'\n",
        "  #                   # Checking if there's a possiblity of lemmatized word be accepted into SWN corpus\n",
        "  #                   try:\n",
        "  #                       this_word_pos=swn.senti_synset(concat).pos_score()\n",
        "  #                       this_word_neg=swn.senti_synset(concat).neg_score()\n",
        "  #                   except Exception as e:\n",
        "  #                       wor = self.pstem.stem(word)\n",
        "  #                       concat = wor+'.'+tag+'.01'\n",
        "  #                       # Checking if there's a possiblity of lemmatized word be accepted\n",
        "  #                       try:\n",
        "  #                           this_word_pos=swn.senti_synset(concat).pos_score()\n",
        "  #                           this_word_neg=swn.senti_synset(concat).neg_score()\n",
        "  #                       except:\n",
        "  #                           self.missing_words.append(word)\n",
        "  #                           continue\n",
        "  #               pos_total+=this_word_pos\n",
        "  #               neg_total+=this_word_neg\n",
        "  #       self.li_swn_pos.append(pos_total)\n",
        "  #       self.li_swn_neg.append(neg_total)\n",
        "\n",
        "  #       if(pos_total!=0 or neg_total!=0):\n",
        "  #           if(pos_total>neg_total):\n",
        "  #               self.li_swn.append(1)\n",
        "  #           else:\n",
        "  #               self.li_swn.append(-1)\n",
        "  #       else:\n",
        "  #           self.li_swn.append(0)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8vDAJgvpDG7T",
        "outputId": "3fd87250-2bcf-4912-d188-4760002b0af9"
      },
      "source": [
        "nltk.download('stopwords')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "eTejtU9vKcgs",
        "outputId": "bf117769-850f-4e67-8ecf-8701b08f99bf"
      },
      "source": [
        "df.head(20)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>username</th>\n",
              "      <th>description</th>\n",
              "      <th>location</th>\n",
              "      <th>following</th>\n",
              "      <th>followers</th>\n",
              "      <th>totaltweets</th>\n",
              "      <th>retweetcount</th>\n",
              "      <th>text</th>\n",
              "      <th>hashtags</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>PANGKAKISTOS</td>\n",
              "      <td>Œ£Œ§ŒóŒù Œ£ŒóŒúŒïŒ°ŒôŒùŒó ŒöŒ°ŒôŒ£Œó Œ§ŒóŒ£ ŒßŒ©Œ°ŒëŒ£ Œó ŒóŒõŒôŒòŒôŒüŒ§ŒóŒ§Œë ŒöŒëŒô...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2451</td>\n",
              "      <td>2248</td>\n",
              "      <td>83222</td>\n",
              "      <td>1</td>\n",
              "      <td>highway TO hell covid covid corona coronavirus...</td>\n",
              "      <td>coronavirus,coronavirusoutbreak,corona,covid_1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>corona_scotnews</td>\n",
              "      <td>The latest #coronavirus #COVID19 news from @dc...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>614</td>\n",
              "      <td>15420</td>\n",
              "      <td>0</td>\n",
              "      <td>nearli half north east given first dose covid ...</td>\n",
              "      <td>coronavirus,covid19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>DailyNews_lk</td>\n",
              "      <td>Sri Lanka's premier daily newspaper, since 1918.</td>\n",
              "      <td>Lake House, Sri Lanka</td>\n",
              "      <td>80</td>\n",
              "      <td>6255</td>\n",
              "      <td>23093</td>\n",
              "      <td>0</td>\n",
              "      <td>covid posit case report covid coronaviru covid...</td>\n",
              "      <td>lakehousedigital,coronavirus,srilanka,covid,co...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>sadia_mufti</td>\n",
              "      <td>Fashion Designer/Stylist | Owner - Hangers The...</td>\n",
              "      <td>Kashmir</td>\n",
              "      <td>102</td>\n",
              "      <td>2559</td>\n",
              "      <td>1256</td>\n",
              "      <td>0</td>\n",
              "      <td>seem corona love shock woman In rajasthan test...</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>Ociocentez</td>\n",
              "      <td>¬°Que a todo el mundo le vaya bien!</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1358</td>\n",
              "      <td>293</td>\n",
              "      <td>45421</td>\n",
              "      <td>9</td>\n",
              "      <td>week said world brink catastroph moral failur ...</td>\n",
              "      <td>corona,coronavirus,pandemic,vaccine,inequality</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5</td>\n",
              "      <td>krishna444_test</td>\n",
              "      <td>Alerting the world, with the current happening...</td>\n",
              "      <td>Karlsruhe, Germany</td>\n",
              "      <td>36</td>\n",
              "      <td>95</td>\n",
              "      <td>10370</td>\n",
              "      <td>0</td>\n",
              "      <td>corona info usa new case today recoveri today ...</td>\n",
              "      <td>covid19,usa,coronavirus,staysafe,corona,telegr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>6</td>\n",
              "      <td>corona_tweet</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>248</td>\n",
              "      <td>33611</td>\n",
              "      <td>24</td>\n",
              "      <td>covid senior doctor call england chief medic o...</td>\n",
              "      <td>coronavirus,covid19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>7</td>\n",
              "      <td>AnthonyEhilebo</td>\n",
              "      <td>Papa Adesuwa and Arese!\\nBuckingham Alumnus\\nB...</td>\n",
              "      <td>Abuja</td>\n",
              "      <td>5203</td>\n",
              "      <td>24146</td>\n",
              "      <td>184219</td>\n",
              "      <td>1</td>\n",
              "      <td>stay busi involv link step stabil busi build r...</td>\n",
              "      <td>businesstip,coronavirus,covidsecondwave,busine...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>9</td>\n",
              "      <td>EightStats</td>\n",
              "      <td>Statistics of topics, which already changed or...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>16</td>\n",
              "      <td>0</td>\n",
              "      <td>op usa state number coronaviru vaccin covid va...</td>\n",
              "      <td>moderna,comirnaty,lockdown,vaccination,pfizer,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>10</td>\n",
              "      <td>corona_scotnews</td>\n",
              "      <td>The latest #coronavirus #COVID19 news from @dc...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>614</td>\n",
              "      <td>15420</td>\n",
              "      <td>0</td>\n",
              "      <td>nearli half north east given first dose covid ...</td>\n",
              "      <td>coronavirus,covid19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>11</td>\n",
              "      <td>EuroSavant</td>\n",
              "      <td>Commentary on the European non-English-languag...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>191</td>\n",
              "      <td>479</td>\n",
              "      <td>36824</td>\n",
              "      <td>0</td>\n",
              "      <td>see whi would micro chip get st corona shot le...</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>13</td>\n",
              "      <td>yespunjab</td>\n",
              "      <td>https://t.co/L8t5kU2S6h #News #Entertainment #...</td>\n",
              "      <td>Punjab</td>\n",
              "      <td>150035</td>\n",
              "      <td>112581</td>\n",
              "      <td>291092</td>\n",
              "      <td>0</td>\n",
              "      <td>chief thank modi continu support covid respon ...</td>\n",
              "      <td>worldhealthorganization,tedrosadhanomghebreyes...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>14</td>\n",
              "      <td>mp320603</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>599</td>\n",
              "      <td>274</td>\n",
              "      <td>23418</td>\n",
              "      <td>112</td>\n",
              "      <td>scientist doctor amp lead author call increa v...</td>\n",
              "      <td>corona,vitaminad,coronavirus,covid,vitamind</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>17</td>\n",
              "      <td>WorldCOVID19</td>\n",
              "      <td>News and information, from around the world.</td>\n",
              "      <td>United Kingdom</td>\n",
              "      <td>725</td>\n",
              "      <td>1059</td>\n",
              "      <td>24380</td>\n",
              "      <td>0</td>\n",
              "      <td>A novel coronaviru cov new strain coronaviru d...</td>\n",
              "      <td>coronavirus,covid19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>18</td>\n",
              "      <td>ArthurBenta</td>\n",
              "      <td>Financial Market Investment\\nSince 1995. \\nSto...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>698</td>\n",
              "      <td>602</td>\n",
              "      <td>773</td>\n",
              "      <td>0</td>\n",
              "      <td>oil fall china covid case high crude build pet...</td>\n",
              "      <td>economy,commodity,finances,petrolprice,invest,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>24</td>\n",
              "      <td>krishna444_test</td>\n",
              "      <td>Alerting the world, with the current happening...</td>\n",
              "      <td>Karlsruhe, Germany</td>\n",
              "      <td>36</td>\n",
              "      <td>95</td>\n",
              "      <td>10370</td>\n",
              "      <td>0</td>\n",
              "      <td>corona info nepal new case today recoveri toda...</td>\n",
              "      <td>covid19,coronavirus,staysafe,corona,telegram,n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>25</td>\n",
              "      <td>CoronaScanner</td>\n",
              "      <td>Corona Scanner is an online dashboard which of...</td>\n",
              "      <td>Eindhoven</td>\n",
              "      <td>83</td>\n",
              "      <td>25073</td>\n",
              "      <td>50114</td>\n",
              "      <td>1</td>\n",
              "      <td>total coronaviru covid infect mexico within in...</td>\n",
              "      <td>coronavirus</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>26</td>\n",
              "      <td>Indonesia_Expat</td>\n",
              "      <td>Indonesia's Largest Expatriate Readership. \\n\\...</td>\n",
              "      <td>Indonesia</td>\n",
              "      <td>440</td>\n",
              "      <td>2062</td>\n",
              "      <td>3886</td>\n",
              "      <td>0</td>\n",
              "      <td>man charg handl indonesia respon covid test po...</td>\n",
              "      <td>ctvalue,donimonardo,corona,covid19,positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>27</td>\n",
              "      <td>CeylonToday</td>\n",
              "      <td>Tweet-length latest news alerts, updates and f...</td>\n",
              "      <td>Colombo, Sri Lanka</td>\n",
              "      <td>102</td>\n",
              "      <td>42920</td>\n",
              "      <td>38257</td>\n",
              "      <td>1</td>\n",
              "      <td>covid case discov baptist church mattakuliya l...</td>\n",
              "      <td>covid19sl,coronavirus,srilanka,corona,covid19l...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>28</td>\n",
              "      <td>Cactus25237334</td>\n",
              "      <td>https://t.co/S7TAcBuDIc /lisa.cactie</td>\n",
              "      <td>Bruxelles, Belgique</td>\n",
              "      <td>243</td>\n",
              "      <td>225</td>\n",
              "      <td>30669</td>\n",
              "      <td>1</td>\n",
              "      <td>corona viru year covid covid coronaviru covid ...</td>\n",
              "      <td>viruschines,virus,coronavirus,covid,covid19,pa...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    index  ...                                           hashtags\n",
              "0       0  ...  coronavirus,coronavirusoutbreak,corona,covid_1...\n",
              "1       1  ...                                coronavirus,covid19\n",
              "2       2  ...  lakehousedigital,coronavirus,srilanka,covid,co...\n",
              "3       3  ...                                                   \n",
              "4       4  ...     corona,coronavirus,pandemic,vaccine,inequality\n",
              "5       5  ...  covid19,usa,coronavirus,staysafe,corona,telegr...\n",
              "6       6  ...                                coronavirus,covid19\n",
              "7       7  ...  businesstip,coronavirus,covidsecondwave,busine...\n",
              "8       9  ...  moderna,comirnaty,lockdown,vaccination,pfizer,...\n",
              "9      10  ...                                coronavirus,covid19\n",
              "10     11  ...                                                   \n",
              "11     13  ...  worldhealthorganization,tedrosadhanomghebreyes...\n",
              "12     14  ...        corona,vitaminad,coronavirus,covid,vitamind\n",
              "13     17  ...                                coronavirus,covid19\n",
              "14     18  ...  economy,commodity,finances,petrolprice,invest,...\n",
              "15     24  ...  covid19,coronavirus,staysafe,corona,telegram,n...\n",
              "16     25  ...                                        coronavirus\n",
              "17     26  ...        ctvalue,donimonardo,corona,covid19,positive\n",
              "18     27  ...  covid19sl,coronavirus,srilanka,corona,covid19l...\n",
              "19     28  ...  viruschines,virus,coronavirus,covid,covid19,pa...\n",
              "\n",
              "[20 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrvym4WlNa3I"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N8sqATnqKldW"
      },
      "source": [
        "df_cleaned.to_excel('/drive/My Drive/cleaned.xlsx')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HeYv629UKmxi",
        "outputId": "aba9ce6d-4c61-49b2-a771-56b1c6af7ae4"
      },
      "source": [
        "from afinn import Afinn\n",
        "from textblob import TextBlob\n",
        "import re\n",
        "nltk.download('vader_lexicon')\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "\n",
        "class polarity_building:\n",
        "  def __init__(self,df):\n",
        "    self.df = df\n",
        "    self.af = Afinn()\n",
        "    self.sid = SentimentIntensityAnalyzer()\n",
        "    self.lem = WordNetLemmatizer()\n",
        "    self.pstem = PorterStemmer()\n",
        "    self.stopwords = stopwords.words('english')\n",
        "    \n",
        "  def clean_part(self,df):\n",
        "    for i in range(len(df)):\n",
        "      txt = df['text'][i]\n",
        "      txt=re.sub(r'@[A-Z0-9a-z_:]+','',txt)#replace username-tags\n",
        "      txt=re.sub(r'^[RT]+','',txt)#replace RT-tags\n",
        "      txt = re.sub('https?://[A-Za-z0-9./]+','',txt)#replace URLs\n",
        "      txt=re.sub(\"[^a-zA-Z]\", \" \",txt)#replace hashtags\n",
        "      \n",
        "      df.at[i,'text']=txt\n",
        "    return df\n",
        "\n",
        "  def clean_full(self,df):\n",
        "    for i in range(len(df)):\n",
        "      text = df['text'][i]\n",
        "      tokens = nltk.word_tokenize(text)\n",
        "      tokens = [word for word in tokens if word not in self.stopwords]\n",
        "      for j in range(len(tokens)):\n",
        "          tokens[j] = self.lem.lemmatize(tokens[j])\n",
        "          tokens[j] = self.pstem.stem(tokens[j])\n",
        "\n",
        "      tokens_sent=' '.join(tokens)\n",
        "      df.at[i,\"text\"] = tokens_sent\n",
        "    return df\n",
        "\n",
        "  def process(self,s):\n",
        "    \n",
        "    if s == 'swn':\n",
        "      self.df = self.sentiment_swn(self.clean_part(self.df))\n",
        "      return\n",
        "    elif s == 'afinn':\n",
        "      \n",
        "      self.df = self.sentiment_afinn(self.clean_full(self.clean_part(self.df)))\n",
        "      return\n",
        "    elif s == 'blob':\n",
        "      \n",
        "      self.df = self.sentiment_blob(self.clean_full(self.clean_part(self.df)))\n",
        "      return\n",
        "    elif s == 'vader':\n",
        "      self.df = self.sentiment_vader(self.clean_full(self.clean_part(self.df)))\n",
        "      return\n",
        "    \n",
        "\n",
        "  def get_df(self):\n",
        "    return self.df\n",
        "\n",
        "  def sentiment_afinn(self,df):\n",
        "    sentiment_scores = [self.af.score(article) for article in list(df['text'])]\n",
        "    sentiment_category = [1 if score > 0 else -1 if score < 0 else 0 for score in sentiment_scores]\n",
        "    df.insert(5,\"sentiment_category\",sentiment_category,True)\n",
        "    return df\n",
        "    \n",
        "\n",
        "\n",
        "  def sentiment_blob(self,df):\n",
        "    sentiment_scores_tb = [round(TextBlob(article).sentiment.polarity, 3) for article in df['text']]\n",
        "    sentiment_category_tb = [1 if score > 0 else -1 if score < 0 else 0 for score in sentiment_scores_tb]\n",
        "    df.insert(5,\"sentiment_category\",sentiment_category_tb,True)\n",
        "    return df\n",
        "\n",
        "  def sentiment_vader(self,df):\n",
        "    compound = [self.sid.polarity_scores(text)['compound'] for text in df['text']]\n",
        "    sentiment_category_vader = [1 if score > 0 else -1 if score < 0 else 0 for score in compound]\n",
        "    df.insert(5,\"sentiment_category\",sentiment_category_vader,True)\n",
        "    return df\n",
        "\n",
        "\n",
        "  def sentiment_swn(self,df):\n",
        "    li_swn=[]\n",
        "    li_swn_pos=[]\n",
        "    li_swn_neg=[]\n",
        "    missing_words=[]\n",
        "    \n",
        "    for i in range(len(df.index)):\n",
        "        text = df['text'][i]\n",
        "        tokens = nltk.word_tokenize(text)\n",
        "        tagged_sent = nltk.pos_tag(tokens)\n",
        "        store_it = [(word, nltk.map_tag('en-ptb', 'universal', tag)) for word, tag in tagged_sent]\n",
        "        #print(\"Tagged Parts of Speech:\",store_it)\n",
        "        \n",
        "        pos_total=0\n",
        "        neg_total=0\n",
        "        for word,tag in store_it:\n",
        "            if(tag=='NOUN'):\n",
        "                tag='n'\n",
        "            elif(tag=='VERB'):\n",
        "                tag='v'\n",
        "            elif(tag=='ADJ'):\n",
        "                tag='a'\n",
        "            elif(tag=='ADV'):\n",
        "                tag = 'r'\n",
        "            else:\n",
        "                tag='nothing'\n",
        "\n",
        "            if(tag!='nothing'):\n",
        "                concat = word+'.'+tag+'.01'\n",
        "                try:\n",
        "                    this_word_pos=swn.senti_synset(concat).pos_score()\n",
        "                    this_word_neg=swn.senti_synset(concat).neg_score()\n",
        "                    #print(word,tag,':',this_word_pos,this_word_neg)\n",
        "                except Exception as e:\n",
        "                    wor = self.lem.lemmatize(word)\n",
        "                    concat = wor+'.'+tag+'.01'\n",
        "                    # Checking if there's a possiblity of lemmatized word be accepted into SWN corpus\n",
        "                    try:\n",
        "                        this_word_pos=swn.senti_synset(concat).pos_score()\n",
        "                        this_word_neg=swn.senti_synset(concat).neg_score()\n",
        "                    except Exception as e:\n",
        "                        wor = self.pstem.stem(word)\n",
        "                        concat = wor+'.'+tag+'.01'\n",
        "                        # Checking if there's a possiblity of lemmatized word be accepted\n",
        "                        try:\n",
        "                            this_word_pos=swn.senti_synset(concat).pos_score()\n",
        "                            this_word_neg=swn.senti_synset(concat).neg_score()\n",
        "                        except:\n",
        "                            missing_words.append(word)\n",
        "                            continue\n",
        "                pos_total+=this_word_pos\n",
        "                neg_total+=this_word_neg\n",
        "        li_swn_pos.append(pos_total)\n",
        "        li_swn_neg.append(neg_total)\n",
        "\n",
        "        if(pos_total!=0 or neg_total!=0):\n",
        "            if(pos_total>neg_total):\n",
        "                li_swn.append(1)\n",
        "            else:\n",
        "                li_swn.append(-1)\n",
        "        else:\n",
        "            li_swn.append(0)\n",
        "\n",
        "\n",
        "    # self.df.insert(5,\"pos_score\",li_swn_pos,True)\n",
        "    # self.df.insert(6,\"neg_score\",li_swn_neg,True)\n",
        "    df.insert(5,\"sent_category\",li_swn,True)\n",
        "    return df\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z8kRlSzPDnuf",
        "outputId": "e565f45f-3374-41fe-f056-ab0a68c877b9"
      },
      "source": [
        "pip install afinn"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting afinn\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/86/e5/ffbb7ee3cca21ac6d310ac01944fb163c20030b45bda25421d725d8a859a/afinn-0.1.tar.gz (52kB)\n",
            "\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                         | 10kB 10.6MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                   | 20kB 15.5MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä             | 30kB 9.1MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà       | 40kB 7.9MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 51kB 4.4MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 61kB 2.9MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: afinn\n",
            "  Building wheel for afinn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for afinn: filename=afinn-0.1-cp36-none-any.whl size=53451 sha256=efbe7543176c912ca5d4de832eec1c64c3733b956116df684dd874f29d2c1e0e\n",
            "  Stored in directory: /root/.cache/pip/wheels/b5/1c/de/428301f3333ca509dcf20ff358690eb23a1388fbcbbde008b2\n",
            "Successfully built afinn\n",
            "Installing collected packages: afinn\n",
            "Successfully installed afinn-0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MwOeGnqley9o",
        "outputId": "7cf769e6-f484-43c0-8453-00b4ed976889"
      },
      "source": [
        "len(df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "660"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RqRJMRwMfAoQ"
      },
      "source": [
        "df_copy = df.copy()\n",
        "pol = polarity_building(df_copy)\n",
        "pol.process('blob')\n",
        "df_vader = pol.get_df()\n",
        "df_vader.to_excel('/drive/My Drive/blob_cleaned.xlsx')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "356PdraZfVkV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "k8prR3Dsfrk5",
        "outputId": "983441f8-e6fc-4ecd-ed71-c4cf92ea8770"
      },
      "source": [
        "df_vader"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>username</th>\n",
              "      <th>description</th>\n",
              "      <th>location</th>\n",
              "      <th>following</th>\n",
              "      <th>sentiment_category</th>\n",
              "      <th>followers</th>\n",
              "      <th>totaltweets</th>\n",
              "      <th>retweetcount</th>\n",
              "      <th>text</th>\n",
              "      <th>hashtags</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>PANGKAKISTOS</td>\n",
              "      <td>Œ£Œ§ŒóŒù Œ£ŒóŒúŒïŒ°ŒôŒùŒó ŒöŒ°ŒôŒ£Œó Œ§ŒóŒ£ ŒßŒ©Œ°ŒëŒ£ Œó ŒóŒõŒôŒòŒôŒüŒ§ŒóŒ§Œë ŒöŒëŒô...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2451</td>\n",
              "      <td>0</td>\n",
              "      <td>2248</td>\n",
              "      <td>83222</td>\n",
              "      <td>1</td>\n",
              "      <td>highway TO hell covid covid corona coronavirus...</td>\n",
              "      <td>coronavirus,coronavirusoutbreak,corona,covid_1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>corona_scotnews</td>\n",
              "      <td>The latest #coronavirus #COVID19 news from @dc...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>614</td>\n",
              "      <td>15420</td>\n",
              "      <td>0</td>\n",
              "      <td>nearli half north east given first dose covid ...</td>\n",
              "      <td>coronavirus,covid19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>DailyNews_lk</td>\n",
              "      <td>Sri Lanka's premier daily newspaper, since 1918.</td>\n",
              "      <td>Lake House, Sri Lanka</td>\n",
              "      <td>80</td>\n",
              "      <td>0</td>\n",
              "      <td>6255</td>\n",
              "      <td>23093</td>\n",
              "      <td>0</td>\n",
              "      <td>covid posit case report covid coronaviru covid...</td>\n",
              "      <td>lakehousedigital,coronavirus,srilanka,covid,co...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>sadia_mufti</td>\n",
              "      <td>Fashion Designer/Stylist | Owner - Hangers The...</td>\n",
              "      <td>Kashmir</td>\n",
              "      <td>102</td>\n",
              "      <td>1</td>\n",
              "      <td>2559</td>\n",
              "      <td>1256</td>\n",
              "      <td>0</td>\n",
              "      <td>seem corona love shock woman In rajasthan test...</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>Ociocentez</td>\n",
              "      <td>¬°Que a todo el mundo le vaya bien!</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1358</td>\n",
              "      <td>1</td>\n",
              "      <td>293</td>\n",
              "      <td>45421</td>\n",
              "      <td>9</td>\n",
              "      <td>week said world brink catastroph moral failur ...</td>\n",
              "      <td>corona,coronavirus,pandemic,vaccine,inequality</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>655</th>\n",
              "      <td>993</td>\n",
              "      <td>DuaSahildua1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>69</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>2676</td>\n",
              "      <td>0</td>\n",
              "      <td>check blog know differ covid vaccin make get v...</td>\n",
              "      <td>covid__19,vaccinationdrive,covid19ireland,vacc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>656</th>\n",
              "      <td>995</td>\n",
              "      <td>AsharqPlus</td>\n",
              "      <td>Bringing you the latest headlines from the aro...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>72</td>\n",
              "      <td>0</td>\n",
              "      <td>kazakhstan plan vaccin million peopl almost th...</td>\n",
              "      <td>kazakhstan,corona,alarabiya,covid_19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>657</th>\n",
              "      <td>996</td>\n",
              "      <td>facemask24_shop</td>\n",
              "      <td>‚òÜ Stay healthy! ‚òÜ\\n‚òÜ\\n‚òÜ\\n‚òÜ\\nImpressum, Datensc...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>25</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>30</td>\n",
              "      <td>0</td>\n",
              "      <td>stay healthi corona coronaviru freedom summer ...</td>\n",
              "      <td>helpothers,covid2020,corona,ootd,coronavirus,h...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>658</th>\n",
              "      <td>997</td>\n",
              "      <td>AmandaR14821525</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>23</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>31</td>\n",
              "      <td>0</td>\n",
              "      <td>coronaviru infect surg unprec height covid cor...</td>\n",
              "      <td>biggboss14,economy,coronavaccine,corona,stocks...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>659</th>\n",
              "      <td>999</td>\n",
              "      <td>530postscript</td>\n",
              "      <td>Almost An Afterthought #AI5W</td>\n",
              "      <td>Bakersfield, CA</td>\n",
              "      <td>194</td>\n",
              "      <td>0</td>\n",
              "      <td>391</td>\n",
              "      <td>38338</td>\n",
              "      <td>0</td>\n",
              "      <td>corona coronaviru covid covid even deadlier of...</td>\n",
              "      <td>coronavirus,corona,covid„Éº19</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>660 rows √ó 11 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     index  ...                                           hashtags\n",
              "0        0  ...  coronavirus,coronavirusoutbreak,corona,covid_1...\n",
              "1        1  ...                                coronavirus,covid19\n",
              "2        2  ...  lakehousedigital,coronavirus,srilanka,covid,co...\n",
              "3        3  ...                                                   \n",
              "4        4  ...     corona,coronavirus,pandemic,vaccine,inequality\n",
              "..     ...  ...                                                ...\n",
              "655    993  ...  covid__19,vaccinationdrive,covid19ireland,vacc...\n",
              "656    995  ...               kazakhstan,corona,alarabiya,covid_19\n",
              "657    996  ...  helpothers,covid2020,corona,ootd,coronavirus,h...\n",
              "658    997  ...  biggboss14,economy,coronavaccine,corona,stocks...\n",
              "659    999  ...                        coronavirus,corona,covid„Éº19\n",
              "\n",
              "[660 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 131
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4nm5pApe8je"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dvPOubNp9bIf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRpEI9dE9WPu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v7rNVxfu6gRe"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}