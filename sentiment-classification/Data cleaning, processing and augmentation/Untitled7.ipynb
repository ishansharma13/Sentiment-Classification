{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled7.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fzuKhYKvi972",
        "outputId": "6ce0418f-de4d-42bd-daa8-508baf588ec6"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8KxbtXx0juRD",
        "outputId": "6584bd34-0456-4772-ed9b-ba19f53db886"
      },
      "source": [
        "!pip install pyspark"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/b0/9d6860891ab14a39d4bddf80ba26ce51c2f9dc4805e5c6978ac0472c120a/pyspark-3.1.1.tar.gz (212.3MB)\n",
            "\u001b[K     |████████████████████████████████| 212.3MB 80kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/b6/6a4fb90cd235dc8e265a6a2067f2a2c99f0d91787f06aca4bcf7c23f3f80/py4j-0.10.9-py2.py3-none-any.whl (198kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 46.5MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.1.1-py2.py3-none-any.whl size=212767604 sha256=3f8395ac610fddda6455c6a69e0a16cc605a97a2885c76731767c8ce25cdd586\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/90/c0/01de724414ef122bd05f056541fb6a0ecf47c7ca655f8b3c0f\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9 pyspark-3.1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWYbiIb8jv5G"
      },
      "source": [
        "from pyspark.sql import SparkSession as ss\n",
        "spark = ss.builder.getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zpDTmPc_jyD5"
      },
      "source": [
        "from pyspark.ml.functions import vector_to_array\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "df2_p1 = df.withColumn(\"xs\", vector_to_array(\"USE_embed\")).select([col(\"xs\")[i] for i in range(0,25)])\n",
        "df2_p2 = df.withColumn(\"xs\", vector_to_array(\"USE_embed\")).select([col(\"xs\")[i] for i in range(25,50)])\n",
        "df2_p3 = df.withColumn(\"xs\", vector_to_array(\"USE_embed\")).select([col(\"xs\")[i] for i in range(50,75)])\n",
        "df2_p4 = df.withColumn(\"xs\", vector_to_array(\"USE_embed\")).select([col(\"xs\")[i] for i in range(75,100)])\n",
        "df2_p5 = df.withColumn(\"xs\", vector_to_array(\"USE_embed\")).select([col(\"xs\")[i] for i in range(100,125)])\n",
        "\n",
        "df2_p6 = df.withColumn(\"xs\", vector_to_array(\"USE_embed\")).select([col(\"xs\")[i] for i in range(125,150)])\n",
        "df2_p7 = df.withColumn(\"xs\", vector_to_array(\"USE_embed\")).select([col(\"xs\")[i] for i in range(150,175)])\n",
        "df2_p8 = df.withColumn(\"xs\", vector_to_array(\"USE_embed\")).select([col(\"xs\")[i] for i in range(175,200)])\n",
        "df2_p9 = df.withColumn(\"xs\", vector_to_array(\"USE_embed\")).select([col(\"xs\")[i] for i in range(200,225)])\n",
        "df2_p10 = df.withColumn(\"xs\", vector_to_array(\"USE_embed\")).select([col(\"xs\")[i] for i in range(225,250)])\n",
        "\n",
        "df2_p11 = df.withColumn(\"xs\", vector_to_array(\"USE_embed\")).select([col(\"xs\")[i] for i in range(250,275)])\n",
        "df2_p12 = df.withColumn(\"xs\", vector_to_array(\"USE_embed\")).select([col(\"xs\")[i] for i in range(275,300)])\n",
        "df2_p13 = df.withColumn(\"xs\", vector_to_array(\"USE_embed\")).select([col(\"xs\")[i] for i in range(300,325)])\n",
        "df2_p14 = df.withColumn(\"xs\", vector_to_array(\"USE_embed\")).select([col(\"xs\")[i] for i in range(325,350)])\n",
        "df2_p15 = df.withColumn(\"xs\", vector_to_array(\"USE_embed\")).select([col(\"xs\")[i] for i in range(350,375)])\n",
        "\n",
        "df2_p16 = df.withColumn(\"xs\", vector_to_array(\"USE_embed\")).select([col(\"xs\")[i] for i in range(375,400)])\n",
        "df2_p17 = df.withColumn(\"xs\", vector_to_array(\"USE_embed\")).select([col(\"xs\")[i] for i in range(400,425)])\n",
        "df2_p18 = df.withColumn(\"xs\", vector_to_array(\"USE_embed\")).select([col(\"xs\")[i] for i in range(425,450)])\n",
        "df2_p19 = df.withColumn(\"xs\", vector_to_array(\"USE_embed\")).select([col(\"xs\")[i] for i in range(450,475)])\n",
        "df2_p20 = df.withColumn(\"xs\", vector_to_array(\"USE_embed\")).select([col(\"xs\")[i] for i in range(475,500)])\n",
        "\n",
        "df2_p21 = df.withColumn(\"xs\", vector_to_array(\"USE_embed\")).select([col(\"xs\")[i] for i in range(500,512)])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "utBzuAtXj3tj"
      },
      "source": [
        "from pyspark.sql import *\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "w=Window.orderBy(lit(1))\n",
        "\n",
        "df4=df2_p1.withColumn(\"rn\",row_number().over(w)-1)\n",
        "df3=df.select(\"u_id\",\"final_sent_class\").withColumn(\"rn\",row_number().over(w)-1)\n",
        "\n",
        "df4 = df4.join(df3,[\"rn\"]).drop(\"rn\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3rIRpS58j9ao"
      },
      "source": [
        "l = df4.columns\n",
        "l.insert(0,l.pop(-1))\n",
        "df4 = df4.select(l)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-FDznDqkEvD"
      },
      "source": [
        "import numpy as np\n",
        "num_p1 = np.array(df4.select('*').collect())\n",
        "num_p2 = np.array(df2_p2.select(\"*\").collect())\n",
        "num_p3 = np.array(df2_p3.select(\"*\").collect())\n",
        "num_p4 = np.array(df2_p4.select(\"*\").collect())\n",
        "num_p5 = np.array(df2_p5.select(\"*\").collect())\n",
        "\n",
        "num_p6 = np.array(df2_p6.select(\"*\").collect())\n",
        "num_p7 = np.array(df2_p7.select(\"*\").collect())\n",
        "num_p8 = np.array(df2_p8.select(\"*\").collect())\n",
        "num_p9 = np.array(df2_p9.select(\"*\").collect())\n",
        "num_p10 = np.array(df2_p10.select(\"*\").collect())\n",
        "\n",
        "num_p11 = np.array(df2_p11.select(\"*\").collect())\n",
        "num_p12 = np.array(df2_p12.select(\"*\").collect())\n",
        "num_p13 = np.array(df2_p13.select(\"*\").collect())\n",
        "num_p14 = np.array(df2_p14.select(\"*\").collect())\n",
        "num_p15 = np.array(df2_p15.select(\"*\").collect())\n",
        "\n",
        "num_p16 = np.array(df2_p16.select(\"*\").collect())\n",
        "num_p17 = np.array(df2_p17.select(\"*\").collect())\n",
        "num_p18 = np.array(df2_p18.select(\"*\").collect())\n",
        "num_p19 = np.array(df2_p19.select(\"*\").collect())\n",
        "num_p20 = np.array(df2_p20.select(\"*\").collect())\n",
        "\n",
        "num_p21 = np.array(df2_p21.select(\"*\").collect())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmx6vqBekHaU"
      },
      "source": [
        "rs = np.concatenate((num_p1,num_p2,num_p3,num_p4,num_p5,num_p6,num_p7,num_p8,num_p9,num_p10,num_p11,num_p12,num_p13,num_p14,num_p15,num_p16,num_p17,num_p18,num_p19,num_p20,num_p21),axis = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UX1CN4YFkII7"
      },
      "source": [
        "from json import JSONEncoder\n",
        "import json\n",
        "\n",
        "class NumpyArrayEncoder(JSONEncoder):\n",
        "    def default(self, obj):\n",
        "        if isinstance(obj, np.ndarray):\n",
        "            return obj.tolist()\n",
        "        return JSONEncoder.default(self, obj)\n",
        "\n",
        "with open(\"df_0.json\", \"w\") as write_file:\n",
        "    json.dump(rs, write_file, cls=NumpyArrayEncoder)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}