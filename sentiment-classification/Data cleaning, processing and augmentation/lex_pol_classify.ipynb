{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lex_pol_classify.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XzMb4UhalAo5",
        "outputId": "26815e69-3a4e-48d0-aa29-082fab34c59b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eug9H5DzlKgb",
        "outputId": "da808792-b7df-4ee3-82ac-c72d2a730e35"
      },
      "source": [
        "!pip install pyspark\n",
        "!pip install afinn"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/b0/9d6860891ab14a39d4bddf80ba26ce51c2f9dc4805e5c6978ac0472c120a/pyspark-3.1.1.tar.gz (212.3MB)\n",
            "\u001b[K     |████████████████████████████████| 212.3MB 68kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/b6/6a4fb90cd235dc8e265a6a2067f2a2c99f0d91787f06aca4bcf7c23f3f80/py4j-0.10.9-py2.py3-none-any.whl (198kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 18.3MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.1.1-py2.py3-none-any.whl size=212767604 sha256=18f0f1b19abb43108a6c80de443a1affd1f744ce03bf0668b523b575f4f8a403\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/90/c0/01de724414ef122bd05f056541fb6a0ecf47c7ca655f8b3c0f\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9 pyspark-3.1.1\n",
            "Collecting afinn\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/86/e5/ffbb7ee3cca21ac6d310ac01944fb163c20030b45bda25421d725d8a859a/afinn-0.1.tar.gz (52kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 3.0MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: afinn\n",
            "  Building wheel for afinn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for afinn: filename=afinn-0.1-cp37-none-any.whl size=53451 sha256=f0a69c38999b4491b6053c8d80d3ec01981f5b93be5d54e66962aa6b877f4d57\n",
            "  Stored in directory: /root/.cache/pip/wheels/b5/1c/de/428301f3333ca509dcf20ff358690eb23a1388fbcbbde008b2\n",
            "Successfully built afinn\n",
            "Installing collected packages: afinn\n",
            "Successfully installed afinn-0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sCkotik1liUw",
        "outputId": "229bb282-b027-4dbf-bca1-07300514265f"
      },
      "source": [
        "from pyspark.sql import SparkSession as ss\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.ml.feature import MaxAbsScaler\n",
        "# from pyspark.ml.linalg import Vectors\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.sql.types import FloatType,StringType\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from afinn import Afinn\n",
        "from textblob import TextBlob\n",
        "import nltk\n",
        "nltk.download('vader_lexicon')\n",
        "spark = ss.builder.getOrCreate()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
            "  warnings.warn(\"The twython library has not been installed. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJf7brtalpyt"
      },
      "source": [
        "class lex_anal:\n",
        "  def __init__(self,file):\n",
        "      self.f = file\n",
        "      self.path_read = '/content/drive/My Drive/'\n",
        "      self.path_write = '/content/drive/My Drive/'\n",
        "      self.df = spark.read.option(\"header\",\"true\").csv(self.path_read+self.f,inferSchema = True)\n",
        "\n",
        "  \n",
        "  def process(self):\n",
        "      self.process_score()\n",
        "      self.scale()\n",
        "      self.process_class()\n",
        "      print(\"SAVING\")\n",
        "      self.save()\n",
        "  \n",
        "  \n",
        "  @staticmethod\n",
        "  @udf(returnType=FloatType())\n",
        "  def vader_pol(text):\n",
        "      vader = SentimentIntensityAnalyzer()\n",
        "      return dict(vader.polarity_scores(text))['compound']\n",
        "\n",
        "  @staticmethod\n",
        "  @udf(returnType=FloatType())\n",
        "  def afinn_pol(text):\n",
        "      af = Afinn()\n",
        "      return af.score(text)\n",
        "\n",
        "  @staticmethod\n",
        "  @udf(returnType = FloatType())\n",
        "  def blob_pol(text):\n",
        "      return TextBlob(text).polarity\n",
        "\n",
        "  def process_score(self):\n",
        "      self.df = self.df.withColumn('vader_score',lex_anal.vader_pol('pre_text_vader'))\n",
        "      self.df = self.df.withColumn('afinn_score',lex_anal.afinn_pol('pre_text_all_upd'))\n",
        "      self.df = self.df.withColumn('blob_score',lex_anal.blob_pol('pre_text_all_upd'))\n",
        "\n",
        "  def process_class(self):\n",
        "      self.df = self.df.withColumn('vader_class',lex_anal.classify('vader_score'))\n",
        "      self.df = self.df.withColumn('afinn_class',lex_anal.classify('afinn_score'))\n",
        "      self.df = self.df.withColumn('blob_class',lex_anal.classify('blob_score'))\n",
        "  \n",
        "  def save(self):\n",
        "      self.df.write.mode(\"overwrite\").option(\"header\",\"true\").csv(self.path_write+'part-4')\n",
        "\n",
        "\n",
        "  def scale(self):\n",
        "      columns_to_scale = [\"afinn_score\"]\n",
        "      assemblers = [VectorAssembler(inputCols=[col], outputCol=col + \"_vec\") for col in columns_to_scale]\n",
        "      scalers = [MaxAbsScaler(inputCol=col + \"_vec\", outputCol=col + \"_scaled\") for col in columns_to_scale]\n",
        "      pipeline = Pipeline(stages=assemblers + scalers)\n",
        "      scalerModel = pipeline.fit(self.df)\n",
        "      scaledData = scalerModel.transform(self.df)\n",
        "    # scaledData = scaledData.drop('afinn_score_vec')\n",
        "      unlist = udf(lambda x: float(list(x)[0]), FloatType())\n",
        "      scaledData = scaledData.withColumn('afinn_score_scaled_f',unlist('afinn_score_scaled'))\n",
        "      scaledData = scaledData.drop('afinn_score_scaled','afinn_score_vec','afinn_score')\n",
        "      scaledData = scaledData.withColumnRenamed('afinn_score_scaled_f','afinn_score')\n",
        "      self.df = scaledData\n",
        "\n",
        "\n",
        "  @staticmethod\n",
        "  @udf(returnType = StringType())\n",
        "  def classify(score):\n",
        "      if score>0.5:\n",
        "          return 'VPos'\n",
        "      if score>0 and score<=0.5:\n",
        "          return 'Pos'\n",
        "      if score<0 and score>=-0.5:\n",
        "          return 'Neg'\n",
        "      if score<-0.5:\n",
        "          return 'VNeg'\n",
        "      return 'Neu'\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aLGtxNp2lvuL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bbe373c-8b4f-4d63-dd4f-ab43070aae40"
      },
      "source": [
        "import os\n",
        "os.listdir()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['How to get started with Drive.pdf',\n",
              " 'sentiment140.csv',\n",
              " 'data_preprocess.ipynb',\n",
              " 'sentiment140_cleaned_csv_preprocessed',\n",
              " 'lex_sen',\n",
              " 'Colab Notebooks',\n",
              " 'Untitled0.ipynb']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQVetF5kmDsq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90ccd3c8-dd16-4c9d-cb68-5f3b2c5dca74"
      },
      "source": [
        "os.chdir('/content/drive/My Drive/sentiment140_cleaned_csv_preprocessed')\n",
        "files = [file for file in os.listdir() if file.endswith('.csv')]\n",
        "files"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['part-00000-acd68a15-c739-4cf0-a97b-c470b4925346-c000.csv',\n",
              " 'part-00001-acd68a15-c739-4cf0-a97b-c470b4925346-c000.csv',\n",
              " 'part-00003-acd68a15-c739-4cf0-a97b-c470b4925346-c000.csv',\n",
              " 'part-00002-acd68a15-c739-4cf0-a97b-c470b4925346-c000.csv',\n",
              " 'part-00004-acd68a15-c739-4cf0-a97b-c470b4925346-c000.csv',\n",
              " 'part-00005-acd68a15-c739-4cf0-a97b-c470b4925346-c000.csv']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlIawSx29kSj"
      },
      "source": [
        "df = spark.read.csv('/content/drive/My Drive/part-00004-acd68a15-c739-4cf0-a97b-c470b4925346-c000.csv',header = True,inferSchema = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CTfANhnr_Foq",
        "outputId": "e84a201a-e89d-424e-a281-8ad9db10a7df"
      },
      "source": [
        "l = lex_anal('part-00004-acd68a15-c739-4cf0-a97b-c470b4925346-c000.csv')\n",
        "l.process()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SAVING\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}