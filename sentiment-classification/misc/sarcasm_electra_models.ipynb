{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sarcasm electra models.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k4aBo3ELGLor",
        "outputId": "411c4d0e-12a0-46d6-da64-f2fa6c077d83"
      },
      "source": [
        "from google.colab import drive\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "# from sklearn.metrics import accuracy_score\n",
        "nltk.download('punkt')\n",
        "import codecs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJraSlgs2ZUi"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn import model_selection as ms\n",
        "import json\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn import model_selection\n",
        "from sklearn import preprocessing\n",
        "# from zeugma.embeddings import EmbeddingTransformer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bpdGewmD3MLi",
        "outputId": "9b14e489-12cf-4557-a7ba-ed6fe2ccb590"
      },
      "source": [
        "# from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# df1 = pd.read_csv('/content/drive/MyDrive/sar_use_data.csv')\n",
        "# df2 = pd.read_csv('/content/drive/MyDrive/sar_elec_294k.csv')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1vCIaW_1qXW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91194011-62c1-453c-8b68-99b351d496da"
      },
      "source": [
        "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-04-28 12:58:08--  https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.74.142, 172.67.9.4, 104.22.75.142, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.74.142|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 681808098 (650M) [application/zip]\n",
            "Saving to: ‘wiki-news-300d-1M.vec.zip’\n",
            "\n",
            "wiki-news-300d-1M.v 100%[===================>] 650.22M  15.8MB/s    in 42s     \n",
            "\n",
            "2021-04-28 12:58:50 (15.7 MB/s) - ‘wiki-news-300d-1M.vec.zip’ saved [681808098/681808098]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_aN8Wz1c40ab",
        "outputId": "77f17358-a105-4fa7-d47b-b1cefd98dbb1"
      },
      "source": [
        "!unzip wiki-news-300d-1M.vec.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  wiki-news-300d-1M.vec.zip\n",
            "  inflating: wiki-news-300d-1M.vec   \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqMoQrpE5Ev1"
      },
      "source": [
        "df2 = df2.rename({'ELEC_embed':'embeddings'},axis = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZGm_5Es35X_a"
      },
      "source": [
        "df1.to_csv('/content/drive/MyDrive/sar_use_data.csv',index=False,header=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SIe8ntMn6MHc"
      },
      "source": [
        "df2.to_csv('/content/drive/MyDrive/sar_elec_294k.csv',index=False,header=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ASUsyPa4qUr"
      },
      "source": [
        "class Model:\n",
        "  def __init__(self,file,models):\n",
        "    self.models = models\n",
        "    df = pd.read_csv(file)\n",
        "    df['tokens'] = df['text'].apply(lambda x: list(word_tokenize(x)))\n",
        "    \n",
        "    # if 'USE_embed' in list(df.columns):\n",
        "    #   df = df.drop('Unnamed: 0',axis = 1)\n",
        "    #   df = df.rename({'USE_embed':'embeddings'},axis = 1)\n",
        "    # elif 'ELEC_embed' in list(df.columns):\n",
        "    #   df = df.rename({'ELEC_embed':'embeddings'},axis = 1)\n",
        "    \n",
        "    # d = {1:'VNeg',2:'Neg',3:'Neu',4:'Pos',5:'VPos'}\n",
        "    # df['label'] = df['label'].apply(lambda x: d[x])\n",
        "\n",
        "    # skf = model_selection.StratifiedKFold(n_splits = 5)\n",
        "\n",
        "    # for f, (t_,v_) in enumerate(skf.split(X=df,y=df.label.values)):\n",
        "    #     df.loc[v_,\"kfold\"] = f\n",
        "    self.dfs_train = [df[df.kfold != f].reset_index(drop=True) for f in range(0,5)]\n",
        "    self.dfs_valid = [df[df.kfold == f].reset_index(drop=True) for f in range(0,5)]\n",
        "    self.fasttext_embeddings_index = self.read_fasttext()\n",
        "  \n",
        "\n",
        "\n",
        "  def read_fasttext(self):\n",
        "      embeddings_index = {}\n",
        "      f = codecs.open('wiki-news-300d-1M.vec', encoding='utf-8')\n",
        "      for line in f:\n",
        "          values = line.rstrip().rsplit(' ')\n",
        "          word = values[0]\n",
        "          coefs = np.asarray(values[1:], dtype='float32')\n",
        "          embeddings_index[word] = coefs\n",
        "      f.close()\n",
        "      print('found %s word vectors' % len(embeddings_index))\n",
        "      return embeddings_index\n",
        "\n",
        "  def transform(self,X):\n",
        "    return np.array([\n",
        "            np.mean([self.fasttext_embeddings_index[w] for w in words if w in self.fasttext_embeddings_index] \n",
        "                    or [np.zeros(300)], axis=0)\n",
        "            for words in X\n",
        "        ])\n",
        "    \n",
        "  def prepare_data(self,i):\n",
        "    # mat_tr = np.matrix([x for x in  self.dfs_train[i]['embeddings'].apply(lambda x: json.loads(x))])\n",
        "    # mat_ts = np.matrix([x for x in  self.dfs_valid[i]['embeddings'].apply(lambda x: json.loads(x))])\n",
        "    # mat_tr = np.matrix([x for x in  self.dfs_train[i]['embeddings'].apply(lambda x: json.loads(x))])\n",
        "    # mat_ts = np.matrix([x for x in  self.dfs_valid[i]['embeddings'].apply(lambda x: json.loads(x))])\n",
        "    \n",
        "    # x_tr= np.squeeze(np.asarray(mat_tr))\n",
        "    # x_ts= np.squeeze(np.asarray(mat_ts))\n",
        "\n",
        "    # x_tr = self.glove.transform(self.dfs_train[i]['text'])\n",
        "    # x_ts = self.glove.transform(self.dfs_valid[i]['text'])\n",
        "    x_tr = self.transform(self.dfs_train[i]['tokens'])\n",
        "    x_ts = self.transform(self.dfs_valid[i]['tokens'])\n",
        "    x_tr = preprocessing.normalize(x_tr, norm='l2')\n",
        "    x_ts = preprocessing.normalize(x_ts, norm='l2')\n",
        "    y_tr = self.dfs_train[i].label.values\n",
        "    y_ts = self.dfs_valid[i].label.values\n",
        "    return x_tr,y_tr,x_ts,y_ts\n",
        "  \n",
        "  def run_models(self):\n",
        "    self.all = {}\n",
        "    for mod in self.models:\n",
        "      self.all[str(mod)] = self.pass_model(mod)\n",
        "    \n",
        "  def get_all(self):\n",
        "    return self.all\n",
        "    \n",
        "  def pass_model(self,mod):\n",
        "    acc = []\n",
        "    for i in range(0,5):\n",
        "      x_tr,y_tr,x_ts,y_ts = self.prepare_data(i)\n",
        "      acc.append(self.train(mod,x_tr,y_tr,x_ts,y_ts))\n",
        "    return acc\n",
        "\n",
        "  def train(self,model,x_tr,y_tr,x_ts,y_ts):\n",
        "    if model == MLPClassifier:\n",
        "      mod = model(early_stopping =True)\n",
        "    else:\n",
        "      mod = model()\n",
        "    mod.fit(x_tr,y_tr)\n",
        "    return accuracy_score(y_ts,mod.predict(x_ts))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bsfW_NFFCJhn",
        "outputId": "e1801c77-620a-4f37-bcfa-901bf43d6518"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import RidgeClassifier,LogisticRegression\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "import numpy as np\n",
        "m = Model('/content/drive/MyDrive/sarcasm data/train_sarc_final_with_folds.csv',[LogisticRegression,RidgeClassifier,GaussianNB,MLPClassifier])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "found 999995 word vectors\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lFSQRLg1EF4x",
        "outputId": "087e0f05-6996-4693-c744-a54d40d016b7"
      },
      "source": [
        "import numpy as np\n",
        "m.run_models()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbTSunFaETb9"
      },
      "source": [
        "x = m.get_all()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlKL2S9WEWh0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f97202c9-8282-4830-cf0c-a03bb5ab39f2"
      },
      "source": [
        "print(x)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\"<class 'sklearn.linear_model._logistic.LogisticRegression'>\": [0.8347619047619048, 0.8347959183673469, 0.8340136054421768, 0.8322789115646259, 0.8345068027210885], \"<class 'sklearn.linear_model._ridge.RidgeClassifier'>\": [0.8298469387755102, 0.8298979591836735, 0.8279251700680272, 0.8278061224489796, 0.8287925170068027], \"<class 'sklearn.naive_bayes.GaussianNB'>\": [0.6624829931972789, 0.6548129251700681, 0.6564455782312926, 0.6574829931972789, 0.6589285714285714], \"<class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>\": [0.8593877551020408, 0.858265306122449, 0.8569727891156462, 0.8573299319727891, 0.8586054421768707]}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x49pKKYNQONJ"
      },
      "source": [
        "# x = {\"RandomForestClassifier\": [0.7351530612244898,\n",
        "#   0.7327551020408163,\n",
        "#   0.7333673469387755,\n",
        "#   0.7351700680272109,\n",
        "#   0.7295578231292517],\n",
        "#  \"LogisticRegression\": [0.7698299319727891,\n",
        "#   0.7681292517006802,\n",
        "#   0.7684353741496599,\n",
        "#   0.7683163265306122,\n",
        "#   0.7668027210884354],\n",
        "#  \"RidgeClassifier\": [0.7688605442176871,\n",
        "#   0.7679931972789116,\n",
        "#   0.7678571428571429,\n",
        "#   0.7681972789115646,\n",
        "#   0.7659183673469387],\n",
        "#   \"GaussianNB\": [0.6474659863945578, \n",
        "#                                                0.6485714285714286, \n",
        "#                                                0.6484183673469388, \n",
        "#                                                0.6487244897959183, \n",
        "#                                                0.6417006802721088],\n",
        "#      \"MLPClassifier\": [0.7926020408163266, \n",
        "#                                                                                0.7893877551020408, \n",
        "#                                                                                0.7922959183673469, \n",
        "#                                                                                0.7929931972789116, \n",
        "#                                                                                0.7907993197278912]\n",
        "#      }\n",
        "\n",
        "y = {\"LogisticRegression\": [\n",
        "     0.981938775510204, \n",
        "     0.9820578231292517, \n",
        "     0.9818537414965987, \n",
        "     0.9826700680272109, \n",
        "     0.981904761904762], \n",
        "     \"RandomForestClassifier\": [\n",
        "      0.913078231292517, \n",
        "      0.9146598639455782, \n",
        "      0.9149489795918367, \n",
        "      0.9164625850340136, \n",
        "      0.9129081632653061],\n",
        "     \"RidgeClassifier\": [\n",
        "      0.9492006802721088, \n",
        "      0.9484353741496598, \n",
        "      0.9512755102040816, \n",
        "      0.950595238095238, \n",
        "      0.950952380952381], \n",
        "     \"GaussianNB\": [\n",
        "      0.8434353741496599, \n",
        "      0.8430952380952381, \n",
        "      0.8442517006802721, \n",
        "      0.8463265306122449, \n",
        "      0.8450170068027211], \n",
        "     \"MLPClassifier\": [\n",
        "      0.9898299319727891, \n",
        "      0.9893367346938775, \n",
        "      0.9903061224489796, \n",
        "      0.9904931972789116, \n",
        "      0.990578231292517]}\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQ1BNYVc89mA"
      },
      "source": [
        "x = m.get_all()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zxeix9AopPOG",
        "outputId": "2defa1de-da66-43a1-98e9-5f11f26ef18e"
      },
      "source": [
        "x"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{\"<class 'sklearn.linear_model._logistic.LogisticRegression'>\": [0.5717391304347826,\n",
              "  0.5731811594202899,\n",
              "  0.572731884057971,\n",
              "  0.5696449275362319,\n",
              "  0.5739057971014493],\n",
              " \"<class 'sklearn.linear_model._ridge.RidgeClassifier'>\": [0.5619275362318841,\n",
              "  0.562427536231884,\n",
              "  0.5627101449275362,\n",
              "  0.5631376811594203,\n",
              "  0.5658043478260869],\n",
              " \"<class 'sklearn.naive_bayes.GaussianNB'>\": [0.39305797101449275,\n",
              "  0.3937608695652174,\n",
              "  0.39555072463768115,\n",
              "  0.3927608695652174,\n",
              "  0.3960072463768116],\n",
              " \"<class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>\": [0.6277246376811594,\n",
              "  0.6286884057971015,\n",
              "  0.6264420289855073,\n",
              "  0.624927536231884,\n",
              "  0.6272753623188406]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6iz5sB6YlZmI"
      },
      "source": [
        "with open('/content/drive/MyDrive/sar_use_accuracy.txt','w') as f:\n",
        "  f.write(str(y))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}