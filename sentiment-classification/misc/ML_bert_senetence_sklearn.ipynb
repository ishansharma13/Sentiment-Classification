{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML bert senetence sklearn.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9Ey2Uq592uJ"
      },
      "source": [
        "from google.colab import drive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aLE5lQ6s97z2",
        "outputId": "b55bb092-a2d9-4c14-ba47-e2e142b9d559"
      },
      "source": [
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dzAMui0C9uop",
        "outputId": "9f0290be-bea8-4a05-998c-bb227dcfa720"
      },
      "source": [
        "!wget http://setup.johnsnowlabs.com/colab.sh -O - | bash"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-04-18 07:26:38--  http://setup.johnsnowlabs.com/colab.sh\n",
            "Resolving setup.johnsnowlabs.com (setup.johnsnowlabs.com)... 51.158.130.26\n",
            "Connecting to setup.johnsnowlabs.com (setup.johnsnowlabs.com)|51.158.130.26|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/scripts/colab_setup.sh [following]\n",
            "--2021-04-18 07:26:38--  https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/scripts/colab_setup.sh\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1594 (1.6K) [text/plain]\n",
            "Saving to: ‘STDOUT’\n",
            "\n",
            "-                     0%[                    ]       0  --.-KB/s               setup Colab for PySpark 3.0.2 and Spark NLP 3.0.1\n",
            "-                   100%[===================>]   1.56K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2021-04-18 07:26:38 (1.32 MB/s) - written to stdout [1594/1594]\n",
            "\n",
            "\u001b[K     |████████████████████████████████| 204.8MB 57kB/s \n",
            "\u001b[K     |████████████████████████████████| 153kB 48.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 204kB 22.4MB/s \n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jq9IYHbU-DJX",
        "outputId": "f4840adb-907d-40f4-a96a-0ea97d921997"
      },
      "source": [
        "# import nlu\n",
        "\n",
        "# pipe = nlu.load('spell')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "spellcheck_dl download started this may take some time.\n",
            "Approximate size to download 112.2 MB\n",
            "[OK!]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1OjoQ5N-8GC"
      },
      "source": [
        "# import pandas as pd\n",
        "\n",
        "# df = pd.read_csv('/content/drive/MyDrive/sentiment data/train_sent_final_with_folds.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4xjVgIiBKYc"
      },
      "source": [
        "def spell(x):\n",
        "  return ' '.join(pipe.predict(x,output_level='sentence')['spell'][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5c5e8ONf_WmI"
      },
      "source": [
        "import pandas as pd\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.types import StringType,StructType,StructField,IntegerType\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "import sparknlp\n",
        "from sparknlp.base import *\n",
        "from sparknlp.annotator import *\n",
        "from pyspark.ml import Pipeline\n",
        "import json\n",
        "from sklearn import model_selection\n",
        "import time\n",
        "class BuildModel:\n",
        "  def __init__(self,path):\n",
        "    self.spark = sparknlp.start()\n",
        "    self.init_prep(path)\n",
        "    # self.prepare_data(path)\n",
        "    self.load_pipe_components()\n",
        "    self.create_fin_pipeline()\n",
        "    self.evaluation()\n",
        "    \n",
        "  def process(self):\n",
        "    l = []\n",
        "    for i in range(len(self.fin_tfolds)):\n",
        "      start = time.time()\n",
        "      print('ROUND '+str(i)+' started')\n",
        "      l.append(self.run_training(self.fin_tfolds[i],self.fin_vfolds[i],i))\n",
        "      end = time.time()-start\n",
        "      print('ROUND '+str(i)+' completed, Time taken: '+str(end)+' seconds')\n",
        "      print('\\n')\n",
        "    return l  \n",
        "\n",
        "  def init_prep(self,path):\n",
        "    df = pd.read_csv(path)\n",
        "    df_vp = df[df['final_sent_class'] == 'VPos'].sample(n=1000)\n",
        "    df_p = df[df['final_sent_class'] == 'Pos'].sample(n=1000)\n",
        "    df_neu = df[df['final_sent_class'] == 'Neu'].sample(n=1000)\n",
        "    df_n = df[df['final_sent_class'] == 'Neg'].sample(n=1000)\n",
        "    df_vn = df[df['final_sent_class'] == 'VNeg'].sample(n=1000)\n",
        "\n",
        "    df = pd.concat([df_vp,df_p,df_neu,df_n,df_vn],ignore_index=True).sample(frac=1).reset_index(drop=True)\n",
        "    df = df.drop(['kfold','final_sent_class'],axis=1)\n",
        "    skf = model_selection.StratifiedKFold(n_splits = 5)\n",
        "\n",
        "    for f, (t_,v_) in enumerate(skf.split(X=df,y=df.sentiment.values)):\n",
        "        df.loc[v_,\"kfold\"] = f\n",
        "\n",
        "\n",
        "    tfolds = [df[df.kfold != fold].drop('kfold',axis=1).reset_index(drop=True) for fold in range(5)]\n",
        "    vfolds = [df[df.kfold == fold].drop('kfold',axis=1).reset_index(drop=True) for fold in range(5)]\n",
        "    schema = StructType([StructField('id', StringType(), True),                     \n",
        "                      StructField('text', StringType(), True),\n",
        "                      StructField('label', IntegerType(), True)])\n",
        "                      \n",
        "    self.fin_tfolds = [self.spark.createDataFrame(data,schema) for data in tfolds]\n",
        "    self.fin_vfolds = [self.spark.createDataFrame(data,schema) for data in vfolds]\n",
        "\n",
        "\n",
        "\n",
        "  # def prepare_data(self,path):\n",
        "  #   df = pd.read_csv(path)\n",
        "  #   if 'sarcasm' in list(df.columns):\n",
        "  #     df = df.drop('sarcasm',axis=1)\n",
        "  #     df = df.rename({'sar_num':'target'})\n",
        "  #   elif 'final_sent_class' in list(df.columns):\n",
        "  #     df = df.drop('final_sent_class',axis=1)\n",
        "  #     df = df.rename({'sentiment':'target'})\n",
        "          \n",
        "  #   tfolds = [df[df.kfold != fold].drop('kfold',axis=1).reset_index(drop=True) for fold in range(5)]\n",
        "  #   vfolds = [df[df.kfold == fold].drop('kfold',axis=1).reset_index(drop=True) for fold in range(5)]\n",
        "  #   schema = StructType([StructField('id', StringType(), True),                     \n",
        "  #                     StructField('text', StringType(), True),\n",
        "  #                     StructField('label', IntegerType(), True)])\n",
        "                      \n",
        "  #   self.fin_tfolds = [self.spark.createDataFrame(data,schema) for data in tfolds]\n",
        "  #   self.fin_vfolds = [self.spark.createDataFrame(data,schema) for data in vfolds]\n",
        "    \n",
        "  def load_pipe_components(self):\n",
        "    self.document = DocumentAssembler().setInputCol(\"text\").setOutputCol(\"document\")\n",
        "    # self.tokenizer = Tokenizer().setInputCols([\"document\"]).setOutputCol(\"token\")\n",
        "    # self.spellModel = ContextSpellCheckerModel.pretrained().setInputCols(\"token\").setOutputCol(\"spell\")\n",
        "    # self.lemmatizer = LemmatizerModel.pretrained(name=\"lemma_antbnc\").setInputCols([\"spell\"]).setOutputCol(\"lemma\").setLazyAnnotator(False)\n",
        "    self.sentenceDetector = SentenceDetector().setInputCols([\"document\"]).setOutputCol(\"sentence\")\n",
        "    # self.use = UniversalSentenceEncoder.pretrained().setInputCols([\"sentence\"]).setOutputCol(\"embeddings\")\n",
        "    self.embeddings = BertSentenceEmbeddings.pretrained(\"sent_small_bert_L6_768\", \"en\").setInputCols(\"sentence\").setOutputCol(\"embeddings\")\n",
        "    # self.embeddings = BertEmbeddings.pretrained(\"small_bert_L12_128\", \"en\").setInputCols(\"sentence\", \"lemma\").setOutputCol(\"embeddings\")\n",
        "    # self.sen_emb = SentenceEmbeddings().setInputCols([\"document\",\"embeddings\"]).setOutputCol(\"sen_emb\").setPoolingStrategy(\"AVERAGE\")\n",
        "    self.embeddings_finisher = EmbeddingsFinisher().setInputCols(\"embeddings\").setOutputCols(\"embeddings_vectors\").setCleanAnnotations(True).setOutputAsVector(True)\n",
        "    # self.assembler = VectorAssembler(inputCols=['embeddings_vectors'],outputCol='features')\n",
        "    self.ml = LogisticRegression(featuresCol='input',labelCol='label')\n",
        "    \n",
        "    \n",
        "    #vec_assembler\n",
        "    #ml model\n",
        "\n",
        "  def create_fin_pipeline(self):\n",
        "    \n",
        "    self.nlp_pipeline = Pipeline(stages = [\n",
        "          self.document,\n",
        "          # self.tokenizer,\n",
        "          # self.spellModel,\n",
        "          # self.lemmatizer,\n",
        "          self.sentenceDetector,\n",
        "          # self.use,\n",
        "          self.embeddings,\n",
        "          # self.sen_emb,\n",
        "          self.embeddings_finisher\n",
        "          # self.assembler,\n",
        "          ])\n",
        "    self.ml_pipeline = Pipeline(stages = [self.ml])\n",
        "          \n",
        "  def run_training(self,df_tr,df_val,i):\n",
        "    df_tr_emb = self.nlp_pipeline.fit(df_tr).transform(df_tr)\n",
        "    df_tr_emb = df_tr_emb.select('id',F.explode('embeddings_vectors').alias('input'),'label')\n",
        "    self.model = self.ml_pipeline.fit(df_tr_emb)\n",
        "    df_val_emb = self.nlp_pipeline.fit(df_val).transform(df_val)\n",
        "    df_val_emb = df_val_emb.select('id',F.explode('embeddings_vectors').alias('input'))\n",
        "    df_res = self.model.transform(df_val_emb)\n",
        "    df_eval = df_res.join(df_val,on='id',how='inner').select('label','prediction')\n",
        "    acc = self.evaluator.evaluate(df_eval, {self.evaluator.metricName: \"accuracy\"})\n",
        "    f1 = self.evaluator.evaluate(df_eval, {self.evaluator.metricName: \"f1\"})\n",
        "    weightedPrecision = self.evaluator.evaluate(df_eval, {self.evaluator.metricName: \"weightedPrecision\"})\n",
        "    weightedRecall = self.evaluator.evaluate(df_eval, {self.evaluator.metricName: \"weightedRecall\"})\n",
        "\n",
        "    return {'accuracy': acc,'f1':f1,'weighted precision': weightedPrecision,'weighted recall':weightedRecall}\n",
        "    \n",
        "  def evaluation(self):\n",
        "    self.evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\")\n",
        "    \n",
        "  def save(self):\n",
        "    return self.model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vHAXN0MJEgnh",
        "outputId": "2d6852a4-6cd2-473a-b01c-7594ddf7b44d"
      },
      "source": [
        "bm = BuildModel('/content/drive/MyDrive/sentiment data/train_sent_final_with_folds.csv')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sent_small_bert_L6_768 download started this may take some time.\n",
            "Approximate size to download 240.9 MB\n",
            "[OK!]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hdZx-QufElEL",
        "outputId": "7deea14e-5c8c-404d-d567-92f3ae2a6ad0"
      },
      "source": [
        "x = bm.process()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ROUND 0 started\n",
            "ROUND 0 completed, Time taken: 540.4855098724365 seconds\n",
            "\n",
            "\n",
            "ROUND 1 started\n",
            "ROUND 1 completed, Time taken: 522.435076713562 seconds\n",
            "\n",
            "\n",
            "ROUND 2 started\n",
            "ROUND 2 completed, Time taken: 511.38531374931335 seconds\n",
            "\n",
            "\n",
            "ROUND 3 started\n",
            "ROUND 3 completed, Time taken: 504.6034789085388 seconds\n",
            "\n",
            "\n",
            "ROUND 4 started\n",
            "ROUND 4 completed, Time taken: 503.52339363098145 seconds\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qg3epJ0N3dyM",
        "outputId": "8b677a02-1290-4aaf-bd24-72cde442a04c"
      },
      "source": [
        "x"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'accuracy': 0.55,\n",
              "  'f1': 0.5523674854602701,\n",
              "  'weighted precision': 0.5562822632156375,\n",
              "  'weighted recall': 0.55},\n",
              " {'accuracy': 0.528,\n",
              "  'f1': 0.5287430109696659,\n",
              "  'weighted precision': 0.5298989409225998,\n",
              "  'weighted recall': 0.528},\n",
              " {'accuracy': 0.529,\n",
              "  'f1': 0.5279722125047982,\n",
              "  'weighted precision': 0.5272582010327078,\n",
              "  'weighted recall': 0.529},\n",
              " {'accuracy': 0.542,\n",
              "  'f1': 0.5403536921995353,\n",
              "  'weighted precision': 0.5401369311208017,\n",
              "  'weighted recall': 0.5419999999999999},\n",
              " {'accuracy': 0.558,\n",
              "  'f1': 0.5576061207793044,\n",
              "  'weighted precision': 0.5573814029396452,\n",
              "  'weighted recall': 0.5579999999999999}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    }
  ]
}